---
layout: post
title: 'MATH 137'
date: 2020-04-28
author: Zosuven
color: rgb(255,215,0)
cover: '/pics/math/cover-math137.jpg'
tags: notes math
---
{% include toc.html %}

# Inequation

#### Properties of &#124; $x$ &#124;

1. $&#124;ab&#124; =&#124;a&#124;&#124;b&#124;$

2. $&#124;\frac{a}{b}&#124;=\frac{&#124;a&#124;}{&#124;b&#124;}$

3. $&#124;a&#124;=\sqrt{a^2}$ used for derivatives

4. $-&#124;a&#124;\le a\le &#124;a&#124;$

   (if $c\ge 0$ , then $&#124;a&#124;\le c$ if and only if $-c\le a\le c$ )

5. inequation's transitivity

![](\pics\math\不等式知识补充.PNG)

### Triangle Inequality

$&#124;x+y&#124; \le &#124;x&#124;+&#124;y&#124;$

alternative form:

$&#124;a-b&#124;\le &#124;a-c&#124;+&#124;c-b&#124;$

logical equal to:

$&#124;a&#124;-&#124;b&#124;\le &#124;a-b&#124;$  (ps: $x=b, y = a-b$)

proof:

by Properties of $&#124;x&#124;$ (4),

$-&#124;x&#124;\le x \le &#124;x&#124;$ and $-&#124;y&#124; \le y \le &#124;y&#124;$  add them together

$-&#124;x&#124;-&#124;y&#124; \le x+y \le &#124;x&#124;+&#124;y&#124;$  = >  $-(&#124;x&#124;+&#124;y&#124;)\le x+y \le &#124;x&#124;+&#124;y&#124;$

by Properties of $&#124;x&#124;$ (4),

$&#124;x+y&#124; \le &#124;x&#124;+&#124;y&#124;$



# Sequences

#### Def: limits of sequences:(convergent or divergent sequences)

Formal def:

Convergent:

let $\{a_n\}$ be a sequence,

if for every $\epsilon >0$ , there exists $N\in \mathbb{N}$   such that $&#124;a_n-L&#124;< \epsilon$  whenever $n>N$,

then $L$ is the limit.   

we write $\lim_{n\to \infty} a_n = L$   and say $\{a_n\}$ is convergent and converges to $L$

divergent:

if no such $L$ exists, $\{a_n\}$ is divergent.

#### Def: Divergent sequences

Let $\{ a_n\}$ be a sequence.

 If for every $M>0$, there exits $N\in M$

such that $a_n>M$ , whenever $n>N$, then  $\lim_{n\to \infty}a_n = \infty$   $\{ a_n \}$ is divergent



### Theorem:(uniqueness)

If $\{a_n\}$ is a convergent sequence,

​	then its limits is unique

(ps: find any subsequence of $\{a_n\}$ , its limit $L$ is unchanged)

proof:

Assume $L$ and $M$ are both limits,

let $\epsilon = \frac{&#124;L-M&#124;}{2}$ , since $\lim_{n\to \infty} a_n = L$

by definition there exists $N\in \mathbb{N}$ , such that  $&#124;a_n -L&#124;<\epsilon$  whenever $n>N_1$ ，

since $\lim_{n\to \infty} a_n = M$,  by definition there exists $N\in \mathbb{N}$ such that $&#124;a_n -M&#124;<\epsilon$, whenever $n>N_2$

if $n>\max(N_1, N_2)$

$&#124;L-M&#124;\le &#124;a_n -L&#124;+&#124;M-a_n&#124;$ (alternative from of the triangle inequality)

​               $= &#124;a_n -L&#124;+&#124;a_n -M&#124; $

​               $< \epsilon +\epsilon$

​               $=2\epsilon$

​               $=&#124;L-M&#124;$

so we must have $L=M$



### Theorem

Let $\{ a_n\}$ be a sequence. The following statements are **equivalent**

1. $\lim_{n\to \infty} a_n = L$
2. for every $\epsilon >0$ , the interval $(L-\epsilon, L+\epsilon)$ contains a tail of $\{ a_n\}$
3. for every $\epsilon >0$,  the interval $(L-\epsilon, L+\epsilon)$ contains all    But finitely many terms of $\{ a_n\}$
4. every interval $(a,b)$ containing $L$ contains a tail of $\{ a_n \}$
5. every interval $(a,b)$ containing $L$ contains all   but finitely many terms of $\{ a_n \}$



### Rules for limits of sequence

let $\{a_n\}, \{b_n\}$ be sequences, such that $\lim_{n\to \infty} a_n = L, \lim_{n\to \infty}b_n=M$

1. for any $c\in \mathbb{R}$ , $\lim_{n\to \infty} c=c$

2. for any $c\in \mathbb{R}$ , $\lim_{n\to \infty} ca_n=cL$

3. $\lim_{n\to \infty} (a_n+b_n) =\lim_{n\to \infty}a_n +\lim_{n\to \infty}b_n =L+M$

4. $\lim_{n\to \infty} a_nb_n=LM$

5. If $M\neq 0$, then $\lim_{n\to \infty}\frac{a_n}{b_n} = \frac{L}{M}$

   (if $M=0$ and $\lim_{n\to \infty}\frac{a_n}{b_n}=0$ , then $a_n = 0$ )

   ### Theorem

   let $\{a_n \}$ be a sequence and let$\{b_n\}$ be a sequence such that $\lim_{n\to \infty} b_n=0$

   if $\lim_{n\to \infty} \frac{a_n}{b_n}=L$   (i.e. $\frac{a_n}{b_n}$ is convergent), then $\lim_{n\to \infty} a_n=0$

   proof:

   $\lim_{n\to \infty} a_n=\lim_{n\to \infty}(\frac{a_n}{b_n})b_n=L(0)=0$

   its **contrapositive**: (if $M=0$ and $a_n\neq0$ ,then $\frac{a_n}{b_n}$ divergent)

6. if $a_n\ge 0$ for all $n$ and $\alpha>0$, then $\lim_{n\to \infty} a_n^{\alpha}=L^{\alpha}$

7. for any $k\in \mathbb{Z}^{+}$, $\lim_{n\to \infty} a_{n+k}= L$

8. if $\alpha>0$, $\lim_{n\to \infty} n\alpha=\infty$



### the squeeze theorem

Assume $\{a_n\}, \{b_n\}, \{c_n\}$ are sequences such that $a_n\le b_n\le c_n$

If $\lim_{n\to \infty} a_n=\lim_{n\to \infty} c_n=L$

​	then, $\lim_{n\to \infty} b_n = L$

proof:

let $\epsilon>0$ , since $\lim_{n\to \infty}a_n =L$

there exists $N_1\in \mathbb{N}$, such that if $n> N_1$, then

​	$&#124;a_n-L&#124;<\epsilon$

​	$L-\epsilon<a_n<L+\epsilon$

since $\lim_>{n\to \infty} c_n=L$, there exists $N_2\in \mathbb{N}$ such that if $n>N_2$, then

​	$L-\epsilon<c_n<L+\epsilon$

if $n>max(N_1, N_2)$

​	$L-\epsilon<a_n\le b_n\le c_n <L+\epsilon$

​	$L-\epsilon<b_n<L+\epsilon $

​	$&#124;b_n -L&#124;<\epsilon$



#### Def: bounded sequences

($S$: represents a sequence)

let $S\subset \mathbb{R}$,

if $S$ is **bounded above** and **bounded below** , we say it is **bounded**

 above:

we say that $\alpha$ is an *<u>upper bound</u>* for $S$, if for all $x\in S$ ,we have $x\le \alpha$

if such a number exists, we say $S$ is **bounded above**

below:

we say that $\beta$ is a *<u>lower bound</u>* for $S$ if for all $x\in S$, we have $x\le \beta$

if such a number exists, we say $S$ is **bounded below**

> **Supremum or infimum**:
>
> if $S$ has an <u>upper bound</u>,
>
> ​	then it has a smallest upper bound called the **least upper bound** or **supremum** ,
>
> ​	denotes: $lub(s)$ or $sup(s)$
>
> if $S$ has a <u>lower bound</u>,
>
> ​	then it has a greatest lower called the **greatest lower bound** or **infimum**,
>
> ​	denotes: $glb(s)$ or  $inf(s)$

### Theorem:(bounded sequence)

If $S$ is bounded , then there exists $N\in \mathbb{R}$ such that $S \subset [-N,N]$



### Monotone Converge theorem:

let $\{a_n\}$ be a **non-decreasing (increasing)** sequence,

​	if $\{a_n\}$ is bounded above, then $\lim_{n\to \infty} a_n=lub(\{a_n\})$

​	if $\{a_n\}$ is not bounded above , then $\lim_{n\to \infty} a_n = \infty$

let$\{b_n\}$ is a **non-increasing (decreasing)** sequence,

​	if it is bounded below, then $\lim_{n\to \infty} b_n = glb(\{b_n\})$

​	if it is not bounded below, then $\lim_{n\to \infty} b_n = -\infty$

proof:

(by induction):

step 1:	Base case: Prove for n=1(n=0)

step2:	Inductive hypothesis: assume the result holds for some $k\in \mathbb{Z}$

step3:	Inductive step: use the inductive hypothesis to prove the results holds for $k+1$.





# Functions' limits (at one point)

#### Def: function's limits (at <u>one Point</u>)

let $f(x)$ be a function,

if for every $\epsilon >0$ , there exists $\delta >0$ , such that $&#124;f(x) - L&#124; < \epsilon$    whenever  $ 0<&#124;x-a&#124;<\delta$  

​	then $\lim_{x\to a}f(x) =L$  

remark: what happens of $x=a$ does not affect whether $\lim_{x\to a}f(x)$ exists or its value.

## (性质)IF the function has the limit (at one point) =>...

### Theorem:(uniqueness)

if $\lim_{x\to a}f(x)$ exists, then the limit is unique.



### Theorem: (Arithmetic Rules for limits of functions)

Let $f$ and $g$ be functions and let $a\in \mathbb{R}$.

if $\lim_{x\to a} f(x) = L$ and $\lim_{x\to a}g(x)=M$ both exists, then

1. for any $c \in \mathbb{R}$, $\lim_{x\to a}c=c$
2. for any $c \in \mathbb{R}$, $\lim_{x\to a} cf(x)=cL$
3. $\lim_{x\to a}(f(x)+g(x)) = L+M$
4. $\lim_{x\to a}f(x)g(x)=LM$
5. $\lim_{x\to a} \frac{f(x)}{g(x)} =\frac{L}{M}$ provided $M\neq 0$
6. $lim_{x\to a}(f(x))^\alpha = L^\alpha$ if $L\ge 0$ and $\alpha >0$

### Theorem:

let $f$ and $g$ be functions such that $\lim_{x\to 0}\frac{f(x)}{g(x)}$ exists,

if $\lim_{x\to a}g(x)=0$, then $\lim_{x\to a}f(x)=0$  (proved by AR 4)

## (条件)...=THE progress of finding a function's limit(may DNE)>...

### Finding limits of Rational Functions.

Rational Functions:

​	$\frac{p(x)}{q(x)}$, where $p$ and $q$ are polynomials

$\lim_{x\to a}\frac{p(x)}{q(x)}$ :

1. check $\lim_{x\to a}q(x)$:

   $\neq 0$: 	$\lim_{x\to a}\frac{p(x)}{q(x)}= \frac{p(a)}{q(a)}$

   $=0$:
2. check $\lim_{x\to a} p(x)$ :

   ​				$\neq 0$:	the limits DNE (because  One-side limits)

   ​				$=0$
3. Factor $(x-a)$ out of $p(x)$ and $q(x)$ and turn back to *step 1*

common tricks:

1. $x^2 -y^2 = (x+y)(x-y)$

2. $x^3 - y^3 = (x-y)(x^2 +xy+y^2)$

3. $x^3 +y^3= (x+y)(x^2 - xy+y^2)$

common error:

- not writing the $\lim_{x\to a} $ each time
- not putting in $x=a$ first (simplify the function first)

### L'Hospital's Rule (LHR)

let $a\in \mathbb{R}$, or $a = \pm \infty$ if $f$ and $g$ are functions such that

1. $f$ and $g$ are both differentiable in an open interval containing an except possibly at $a$.
2. $g(x)\neq 0$ for all $x$ in the interval
3. $\lim_{x\to a} \frac{f(x)}{g(x)}$ is of type $\frac{0}{0}$ or $\frac{\infty}{\infty}$ ...
4. $\lim_{x\to a}\frac{f'(x)}{g'(x)}=L$ where $L\in \mathbb{R}$ or $L= \pm \infty$

then $\lim_{x\to a} \frac{f(x)}{g(x)}=L$.

common mistakes

1. use LHR when you do not have an **indeterminate form**
2. not simplifying before using LHR
3. using the quotient rule instead of taking the derivative of the denominator
4. not writing $\lim_{x\to a}$ appropriately

<span style="color:red">✮</span>**Not** indeterminate form: $\frac{0}{\infty} =0$,   $\frac{\infty}{0} =0$ , $0^\infty=0$ , $\infty^0=1$

ex:

1. $type(\frac{0}{0})$

   calculate $\lim_{x\to 1}\frac{\ln x}{x-1}$

   $\overset {LHR}{=} \lim_{x\to 1}\frac{1/x}{1}=1$

2. $type(\frac{\infty}{\infty})$

   calculate $\lim_{x\to \infty} \frac{e^x}{ x^2}$

   $\overset{LHR}{=} \lim_{x\to \infty} \frac{e^x}{2x}$ $type(\frac{\infty}{\infty})$

   $\overset{LHR}{=}\lim_{x\to \infty} \frac{e^x}{2}=\infty$

   calculate $\lim_{x\to \infty} \frac{\ln x}{x^{1/3}}= \lim_{x\to \infty}\frac{1/x}{\frac{1}{3}x^{-2/3}} \overset{LHR}{=} \lim_{x\to \infty} \frac{3}{x^{1/3}}=0$  

3. $type(0\cdot \infty)$

   calculate $\lim_{x\to 0^+}x\ln x = \lim_{x\to 0^+} \frac{\ln x}{1/x}$ ($type(\frac{-\infty}{\infty})$) $\overset{LHR}{=} \lim_{x\to 0^+} \frac{1/x}{-x^{-2}} = \lim_{x\to 0^+}(-x)=0$

   calculate $\lim_{x\to -\infty}x^2 e^x = \lim_{x\to -\infty}\frac{x^2} {e^{-x}}$ ($type(\frac{\infty}{\infty})$) $\overset{LHR}{=}\lim_{x\to -\infty}\frac{2x}{-e^{-x}} \overset{LHR}{=}\lim_{x\to -\infty} \frac{2}{e^{-x}}=0$

4. $type(\infty-\infty)$ (use some strategy to convert to $\frac{0}{0}$ or $\frac{\infty}{\infty}$ )

   calculate $\lim_{x\to \frac{\pi}{2}^-} (\sec x-\tan x)= \lim_{x\to \frac{\pi}{2}^-}(\frac{1}{\cos x}-\frac{\sin x}{\cos x})= \lim_{x\to \frac{\pi}{2}^-}(\frac{1-\sin x}{\cos x})$ ($type(\frac{0}{0})$) $\overset{LHR}{=} \lim_{x\to \frac{\pi}{2}^-}(\frac{-\cos x}{-\sin x})=\frac{0}{1}=0$

5. $type(0^0)$

   calculate $\lim_{x\to 0^+} x^x$

   $= \lim_{x\to 0^+ }(e^{\ln x})^x = \lim_{x\to 0^+ }e^{x\ln x} = e^{\lim_{x\to 0^+ }x\ln x}$ ---(1)

   consider the exponent:

   ​	$\lim_{x\to 0^+ } x\ln x=0$ $ type(0\cdot (-\infty))$ by example above , back into equation (1): $\lim_{x\to 0^+ }x^x =e^0=1$

6. $type(1^{\infty})$

   calculate $\lim_{x\to \infty}(1+\frac{a}{x})^{bx} = (b>0) \lim_{x\to \infty} (e^{\ln (1+\frac{a}{x})})^{bx}= e^{\lim_{x\to \infty}bx\ln (1+\frac{a}{x})}$ ---(2)

   look at exponent:

   ​	$\lim_{x\to \infty} bx \ln (1+\frac{a}{x})= b\cdot \lim_{x\to \infty}\frac{\ln (1+\frac{a}{x})}{x^{-1}}$ ($type(\frac{0}{0})$) $\overset{LHR}{=} b\cdot \lim_{x\to \infty} \frac{\frac{1}{1+\frac{a}{x}}\cdot (\frac{-a}{x^2 })}{-x^{-2}}=ab$

   ​	by equation (2), we have $\lim_{x\to \infty}(1+\frac{a}{x})^{bx} =e^{ab}$  



### Sequential characterization of limits

let $f$ be a function defined on an open interval containing an except possibly at $a$.

then:

​	$\lim_{x\to a}f(x)=L$ if and only if

if $\{x_n\}$ is any sequence with $x_n \neq a$ and $\lim_{n\to \infty} x_n=a$ , then $\lim_{n\to \infty} f(x_n)=L$.

### To prove $\lim_{x\to \infty}f(x)$ does not exist:

Me1:	find a sequence $\{x_n\}$ such that $\lim_{n\to \infty} x_n=a$ and $x_n \neq a$ for which $\lim_{n\to \infty}f(x_n)$ DNE

Me2:	find two sequences $\{x_n\}, \{y_n\}$ with $\lim_{n\to \infty} x_n=a=\lim_{n\to \infty} y_n$, $x_n \neq a, y_n \neq a$ for which $\lim_{n\to \infty}f(x_n) \neq \lim_{n\to \infty}f(y_n)$

#### Def: One_side Limits

let $f$ be a function and $a \in \mathbb{R}$.  

If for every $\epsilon >0$ , there exists $\delta >0$ such that $&#124;f(x)-L&#124;<\epsilon$   whenever $0< x-a<\delta$ ($a<x<a+\delta$)($x$ 比$a$ 大)

​	then $\lim_{x\to a^{+}}f(x)=L$

If for every $\epsilon >0$ , there exists $\delta >0$ such that $&#124;f(x)-L&#124;<\epsilon$   whenever $0< a-x<\delta$ ($a<x<a+\delta$)($x$ 比$a$ 小)

​	then $\lim_{x\to a^{-}}f(x)=L$

### Theorem (One-side):

let $f(x)$ be defined in an open interval containing an except possibly at $a$ , then

​	$\lim_{x\to a} f(x)=L$ if and only if both $\lim_{x\to a^{+}}f(x)=L=\lim_{x\to a^{-}}f(x)$



### The squeeze Theorem

Assume $f, g, h$ functions defined on an open interval $I$ containing an except possibly at $a$,

If for all $x\in I$, we have:

​	$f(x)\le g(x) \le f(x)$ and $\lim_{x\to a}f(x)=L=\lim_{x\to a}h(x)$

​	then $\lim_{x\to a} g(x)=L$



### Fundamental trig limits

$\lim_{x\to 0}\frac{\sin x}{x}=1$ , $\lim_{x\to 0}\frac{x}{\sin x}=1$, $\lim_{x\to 0}\frac{1-\cos x}{x}=0$



#### Def: Limits of Functions equally infinity

let $f$ be a function defined on an interval containing an except possibly at $a$ .

If for all $M>0$ there exists $\delta >0$ such that

​	if $0<&#124;x-a&#124;<\delta$ then  $f(x)>M$,

​		then we write $\lim_{x\to a}f(x)=\infty$

let $f$ be a function defined on an interval containing an except possibly at $a$.

If for all $M<0$ there exists $\delta >0$ such that

​	if $0<&#124;x-a&#124;<\delta$ then $f(x)<M$ ,

​		then we write $\lim_{x\to a}f(x)=-\infty$



# Function's limits (at infinite)

#### Def: Function's limit at infinite

let $\epsilon >0$ there exists $N>0$ , such that

if $x>N$, then $&#124;f(x)-L&#124;<\epsilon$ , then $\lim_{x\to \infty} f(x)=L$

let $\epsilon >0$ , there exists $N>0$ , such that

if $x<N$ , then $&#124;f(x)-L&#124;<\epsilon$ , then $\lim_{x\to -\infty}f(x)=L$



let $M>0$, there exists $N>0$ such that  

if $x>N$, then $f(x)>M$, then $\lim_{x\to \infty}f(x)=\infty$

let $M>0$, there exists $N<0$ such that

if $x<N$, then $f(x)>M$, then $\lim_{x\to -\infty}f(x)=\infty$

let $M<0$, there exists $N>0$ such that

if $x>N$ , then $f(x)<M$ , then $\lim_{x\to \infty}f(x)=-\infty$

let $M<0$, there exists $N<0$ such that

if $x<N$ , then $f(x)<M$, then $\lim_{x\to -\infty}f(x)=-\infty$



## Horizontal or Vertical Asymptote

#### Def: Horizontal or Vertical Asymptote

let $f(x)$ be a function

if $\lim_{x\to \infty}f(x)=L$ or $\lim_{x\to -\infty}f(x)=L$, then we say that

​	the line $y=L$ is a horizontal asymptote

if $\lim_{x\to a^{+}}f(x)=\pm \infty$ or $\lim_{x\to a^{-}}f(x)=\pm \infty$ , then

​	the line $x=a$  is a vertical asymptote



### Fundamental log limit

$\ln(a^{b})=b\ln(a)$

$\ln(ab)=\ln a+\ln b$

$\ln(\frac{a}{b})=\ln a-\ln b$

**theorem:**

​	$\lim_{x\to \infty}\frac{\ln x}{x}=0$

​	it means that $f(x)=\frac{\ln x}{x}$ has a horizontal asymptote of $y=0$

​							the value of $f(x)=x$ grow much faster than the values of $g(x)=\ln x$.

​

$\lim_{x\to \infty}\frac{\ln x}{x^{p}}=0$ for all $p>0$

proof:

observe $\ln x=\frac{p}{p}\ln x= \frac{1}{p}\ln x^{p}$ , so $\lim_{x\to \infty}\frac{\ln x}{x^p}=\lim_{x\to \infty}\frac{1}{p}\frac{\ln x^{p}} {x^{p}}=\frac{1}{p}(0)=0$

**a useful trick**:

ex: Evaluate $\lim_{x\to 2}\frac{\sin(x+2)}{x+2}$

​	sol: let $h=x+2 $

​			as $x\to -2$, then $h\to 0$    $\lim_{x\to -2}\frac{\sin(x+2)}{x+2}= \lim_{h\to 0}\frac{\sin h}{h}=1$



### Theorem ($x\to a$ to $h\to 0$)

if $\lim_{x\to a}f(x)$ exists, then  $\lim_{x\to a}f(x)= \lim_{h\to 0}f(a+h)$



# Continuity

#### Def:

##### continuous at one point:

**Formal Definition of continuity Ⅰ**

a function is continuous at $x=a$ if

1. $\lim_{x\to a}f(x)$ exists

2. $\lim_{x\to a}f(x)=f(a)$

**Formal Definition of continuity Ⅱ**

a function is continuous at $x=a$ if

for every $\epsilon >0$ , there exists $\delta >0$ such that     if $&#124;x-a&#124;<\delta$ , then $&#124;f(x)-f(a)&#124;<\epsilon$  

note: which is different from the definition of limits, because the $f(a)$ means $f(a)$ must exists for continuity

**Formal Definition of continuity Ⅲ**

a function $f$ is continuous at $x=a$ if for every sequence $\{x_n\}$ such that   

if $ \lim_{n\to \infty}x_n=a$ ($x_n$ has the limit $a$ ), then $\lim_{n\to \infty}f(x_n)=f(a)$

**Ⅳ($h\to 0$)**

if $\lim_{h\to 0}f(a+h)=f(a)$, then $f$ is continuous at $x=a$

##### continuous on $(a, b)$

obviously...

a function is said to be continuous on $(a, b)$ , if it is continuous at every $x\in (a, b)$

note: a function that is continuous on $\mathbb{R}$ is sometimes said to be continuous everywhere

##### continuous on $[a, b]$

a function $f$ is said to be continuous on $[a, b]$ , if

1. it is continuous on $(a,b)$
2. $\lim_{x\to a^{+}}f(x)=f(a)$
3. $\lim_{x\to a^{-}}f(x)=f(b)$

##### a continuous function...

def:   A function is said to be continuous if it is continuous at each point in its domain.

Note :

1. all **polynomials** are continuous
2. all **rational functions** are continuous (不连续的地方不在它的定义域里)
3. all **logarithm** and **exponential** functions are continuous

根据定义，分段函数可以做到不连续



### continuity of composition

let $f, g$ be functions and let $h=g\circ f$, which is actually $g(f(x))$

if $f$ is continuous at $x=a$ , and $g$ is continuous at $y=f(a)$ , then $h$ is continuous at $x=a$.

proof:

by the *Formal definition of continuity Ⅲ* ,

since $\lim_{x\to a}f(x)=f(a)$, so for any $\{x_n\}$ such that $\lim_{n\to \infty}x_n=a$, we have $\lim_{n\to \infty}f(x_n)=f(a)$

since $\lim_{y\to f(a)} g(y)=g(f(a))$ and $\{f(x_n)\}$ has limit $f(a)$, we get $\lim_{n\to \infty}g(f(x_n))=g(f(a))$

so, by the formal definition of continuity Ⅲ

$\lim_{x\to a}g(f(x))=g(f(a))$



### continuity of Inverse

assume $f$ is an invertible function with inverse $g$.

if $f$ is continuous at $x=a$ , then $g$ is continuous at $y=f(a)$.



### Intermediate Value Theorem (IVT)

if $f$ is a function that is continuous on $[a, b]$ and either $f(a)<c<f(b)$, or $f(b)<c<f(a)$ ,

then there exists **at least one** $x\in (a, b)$ such that $f(x)=c$ .

![](\pics\math\math137-1.jpg)

# Discontinuity

##### classify discontinuities($f$ will not continuous at $x=a$)

1. if $\lim_{x\to a}f(x)$ does not exists

   ​	a. if $\lim_{x\to a^{+}}f(x)$ or $\lim_{x\to a^{-}}f(x)$ DNE:  **essential discontinuity**

   ​	b. if $\lim_{x\to a^{+}}f(x)$ and $\lim_{x\to a^{-}}f(x)$ exist, but are not equal: **jump discontinuity**

2. if $\lim_{x\to a}f(x)$ exists but dose not equal $f(a)$ or $f(a)$ does not exist, this is called **removable discontinuity**



#  Algorithm: Bisection Method(二分法)

let $f$ be a continuous function, given $\epsilon >0$ , find a point $d$ such that

​	there exists a root $c$ of $f$ satisfying $&#124;c-d&#124;<\epsilon$

step 1: find two points $a_0, b_0$, with $a_0<b_0$ such that $f(a_0)f(b_0)<0$

step 2: let $l= b_0-a_0$ and set counter $n=0$

step 3: let $d_n = \frac{a_n+b_n}{2}$

step 4: If $f(d_n)=0$ or if $\frac{l}{2^{n+1}}<\epsilon$ return $d_n$

step 5: If $f(a_n)f(d_n)<0$  , then let $b_{n+1}=d_n$ , $ a_{n+1}=a_n$ , $n=n+1$

​			else let $a_{n+1} = d_n$ , $b_{n+1} = b_n$ , $n=n+1$

step 6: go to step 3



# Global/absolute Maximum or Minimum

#### Def:(global Max or Min)

let $f$ be a function defined on an interval $I$.

A number $c\in I$ is called a **global Maximum** for $f$ on $I$ , if $f(x)\le f(c)$ for all $x\in I$

A number $d \in I$ is called a **global Minimum** for $f$ on $I$ , if $f(x)\ge f(d)$ for all $x\in I$

we say $f\in I$ is a **global extremum** if it is a global Max or Min

they are also called **absolute** Max or Min



没有global Max or Min:

1. there is an essential discontinuity($\pm \infty$) in the interval $I$
2. it is only continuous in $I=(a,b)$ (开区间)

### Extreme Value Theorem(EVT)

if $f$ is continuous on $[a,b]$ , then there exists $c, d \in[a,b]$  such that

$f(d)\le f(x) \le f(c)$ for all $x\in [a,b]$ . (there must be global Max and Min)

闭区间一定有最值



# Rate of changes

#### Def: (rate of changes)

$f$ over interval $[a,b]$ is defined by:

average rate of change $= \frac{\Delta f}{\Delta x}=\frac{f(b)-f(a)}{b-a}$

| Function     | Rate of change |
| ------------ | -------------- |
| Position     | Velocity       |
| Velocity     | Acceleration   |
| Acceleration | Jerk           |
| Jerk         | Jounce (snap)  |
| Snap         | Crackle        |
| Crackle      | ...            |



#### Def: derivative

let $f$ be a function defined on an open interval containing $a$.

the instantaneous rate of change of $f$ at $x=a$  also called the derivative of $f$ at $x=a$ is defined by

​	$f'(a)= \lim_{h\to 0}\frac{f(a+h)-f(a)}{h}$ or $f'(a)=\lim_{x\to a}\frac{f(x)-f(a)}{x-a}$

if the limit exists. if it does, $f$ is said to be differentiable at $x=a$.



### Def: tangent line

if $f$ is differentiable at $x=a$ , then the <u>tangent line</u> of $f$ at $x=a$ is the line passing through $(a, f(a))$ with slope $m=f'(a)$

in particular, $y=f(a)+f'(a)(x-a)$

过一点$(a,b)$ 的直接方程:

![](\pics\math\doge.jpg)

忘了吧。。。holy shit...

$y-b=k(x-a)$ because, $k=\frac{y-b}{x-a}$

### Theorem(differentiable=>continuous)

if $f$ is differentiable at $x=a$ , then $f$ is continuous at $x=a$ (its converse is **False**)

proof:

$\lim_{x\to a}f(x)=\lim_{x\to a}f(x)-f(a)+f(a)$

​					 since $f'(a)=\lim_{x\to a}\frac{f(x)-f(a)}{x-a}$

​					$=\lim_{x\to a}\frac{f(x)-f(a)(x-a)}{x-a}+f(a) = f'(a)(0)+f(a)=f(a)$

so $f$ is continuous at $x=a$



#### Def: differentiable on an interval

a function$f$ is differentiable on an interval $I$ , if $f'(a)$ exists for each $a\in I$

in this case, we define the derivative function of $f$ on $I$ by $f'(x)= \lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$ for all $x\in I$.

Note: if $I$ is not specified, we mean the domain of the function.

Notation List.

|          | 1st                              | 2nd                  | 3rd                  | nth                  | 1st at $x=a$                           |
| :------: | -------------------------------- | -------------------- | -------------------- | -------------------- | -------------------------------------- |
| Leibniz  | $\frac{dy}{dx}$, $\frac{d}{dx}y$ | $\frac{d^2 y}{dx^2}$ | $\frac{d^3 y}{dx^3}$ | $\frac{d^n y}{dx^n}$ | $\frac{dy(a)}{dx}$, $\frac{dy}{dx}(a)$ |
| Lagrange | $f'$                             | $f^{\prime\prime}$                | $f^{(3)}$            | $f^{(n)}$            | $f'(a)$                                |
|  Euler   | $Df$                             | $D^2f$               | $D^3 f$              | $D^n f$              | $Df(a)$                                |



### Derivative Rules

| Function            | Derivative                   |
| ------------------- | ---------------------------- |
| $f(x)=c$            | $f'(x)=0$                    |
| $n\neq 0, f(x)=x^n$ | $f'(x)=nx^{n-1}$             |
| $f(x)=\cos x$       | $f'(x)=-\sin x$              |
| $f(x)=\sin x$       | $f'(x)=\cos x$               |
| $f(x)=\tan x$       | $f'(x)=\sec^2 x$             |
| $f(x)=\sec x$       | $f'(x)=\sec x \tan x$        |
| $f(x)=\csc x$       | $f'(x)=-\csc x \cot x$       |
| $f(x)=\cot x$       | $f'(x)=-\csc^2 x$            |
| $f(x)=e^x$          | $f'(x)=e^x$                  |
| $f(x)=a^x$          | $f'(x)=a^xln&#124;a&#124;$ |
| $f(x)=\ln x$        | $f'(x)= \frac{1}{x}$         |
| $f(x)=\log_a x$     | $f'(x)= \frac{1}{x \ln a}$   |



### Arithmetic Rules for derivative

if $f$ and $g$ are differentiable at $x=a$, then

1. if $h(x)=cf(x)$, then $h'(a)=cf'(a)$ 	(Constant Multiple Rule)
2. if $h(x)=f(x)+g(x)$, then $h'(a)=f'(a)+g'(a)$      (Sum Rule)
3. if $h(x)=f(x)g(x)$, then $h'(a)=f'(a)g(a)+f(a)g'(a)$    (Product Rule)
4. if $h(x)=\frac{f(x)}{g(x)}$ , $g'(a)\neq 0$ , $h'(a)=\frac{f'(a)g(a)-f(a)g'(a)}{(g(a))^2}$     (Quotient Rule)

proof of Product Rule:

​	$(fg)'=\lim_{h\to 0}\frac{(fg)(x+h)-(fg)(x)}{h}=\lim_{h\to 0}\frac{f(x+h)g(x+h)-f(x)g(x)}{h}$

​			$=\lim_{h\to 0}\frac{f(x+h)g(x+h)-f(x)g(x+h)+f(x)g(x+h)-f(x)g(x)}{h}$

​			$=\lim_{h\to 0}[g(x+h)(\frac{f(x+h)-f(x)}{h})+f(x)(\frac{g(x+h)-g(x)}{h})]$  

​			$=g(x)f'(x)+f(x)g'(x)$

proof of Quotient Rule:

​	$(\frac{f}{g})'=\lim_{h\to 0}(\frac{f(x+h)}{g(x+h)}-\frac{f(x)}{g(x)})\frac{1}{h}=\lim_{h\to 0}\frac{g(x)f(x+h)-g(x+h)f(x)}{g(x+h)g(x)h}$

​			$=\lim_{h\to 0}\frac{g(x)f(x+h)-g(x)f(x)+g(x)f(x)-g(x+h)f(x)}{g(x+h)g(x)h}$

​			$=\lim_{h\to 0}\frac{1}{g(x+h)g(x)}[g(x)(\frac{f(x+h)-f(x)}{h})-f(x)(\frac{g(x+h)-g(x)}{h})]$

​			$=(\frac{1}{g(x)})^2 [g(x)f'(x)-f(x)g'(x)]$

##### Chain Rule

if $f$ is a differentiable at $x=a$ and $g$ is differentiable at $y=f(a)$,

​	then $h=g(f)$ is differentiable at $x=a$ and $h'(a)=g'(f(a))f'(a)$

​	for the derivative function

​	$h'=g'(f)f'$  which is $\frac{dy}{dx}=\frac{dy}{du} \cdot \frac{du}{dx}$  where $y=g(u) , u=f(x)$



# Linear approximation

#### def: linear approximation

let $f$ be differentiable at $x=a$

the linear approximation (linearization)of $f$ at $x=a$ is

$ L_a^f(x)=f(a)+f'(a)(x-a)$  		for $x$ close to $a$ , we have $f(x)\approx L_a^f(x)$

note:

1. $L_a^f(x)$ represents a linear function
2. it is actually the tangent line of $f$ at $x=a$

### theorem: linear approximation of a differential function at $a$

if $f$ is differentiable at $x=a$, then

1. $L_{a}^{f}(a)=f(a)$
2. $(L_a^f)'(a)=f'(a)$
3. $L_a^f$ is the **only linear function** with properties

note:

this is because the properties show how linear approximation forms



### Error in the linear approximation

let $f$ be a function differentiable at $x=a$

3 factors that affect how accurate of an approximation $L_{a}^{f} (x)$ gives:

1. how close $x$  is to $a$
2. how curvy the function is
<br>how curvy a function depends on $f^n$
3. if $f$ is continuous on $[x,a]$

note:

1. if the graph is concave up ($f^{\prime\prime}>0$), then $L_{a}^{f} (x)$ is an underestimate
2. if the graph is concave down($f^{\prime\prime}<0$), then ... is an overestimate



### Theorem: "good approximation"

let $f$ be a function that is differentiable at $x=a$,

then $\lim_{x\to a}\frac{&#124;f(x)-L_{a}^f(x)&#124;}{&#124;x-a&#124;}=0$   (the error goes to $0$ faster than the distance from $x$ to $a$ )

in which error $=&#124;f(x)-L_{a}^f (x)&#124;$

we define this to be a "good approximation"

$L_{a}^f (x)$ is the only linear function with the property.



### Error in the linear Approximation (EITLA)

assume $f$ is twice differentiable in an open interval $I$ containing $a$.

if $&#124;f^{\prime\prime}(x)&#124;\le M$ for all $x\in I$ , then

$&#124;f(x)-L_{a}^f(x)&#124; \le \frac{M}{2}(x-a)^2$ for all $x \in I$



#### def: Estimating change

from the linear approximation we know

$f(x)\approx f(a)+f'(a)(x-a)$

$f(x)-f(a)\approx f'(a)(x-a)$

change in $f$ 	change in $x$

$\Delta f \approx f'(a)\Delta x$

note: $\Delta x$ and $\Delta f$ are called differentials



### Newton's Method

purpose:

for approximating roots of functions.

Assume $f$ is differentiable on $[a,b]$ containing a root of $f$.

let $x_1$ is a "good" estimate of $r$

let $h=r-x_1$ ,

​	since $h$ is 'small'       ps: linear approximation: $a=x_1, x=x_1+h$

​	$0=f(r)=f(h+x_1)\approx L_{x_1}{f}(x_1+h)=f(x_1)+f'(x_1)(x_1+h-x_1)$

$-\frac{f(x_1)}{f'(x_1)} \approx h$   we can get a new approximation $x_2$

​	$ x_2 = x_1-\frac{f(x_1)}{f'(x_1)}$  since $r \approx x_1+h$

we get a sequence that is recursive

<span style="color:red">✮</span> 	$x_{n+1}=x_n - \frac{f(x_n)}{f'(x_n)}$

Newton's Method

##### problems of Newton's Method

Main problem: It does not always work

Note:

If we choose or get $x_i$ such that $f'(x_i)=0$,

​	then newton's method fails, or ($f'$ does not exists or $f$ does not exists)

$x_i = x_j$ for some $i,j$

if $f(x_i)=0$ where $x_i$ is not the root you are looking for.



# Inverse function

#### Def:

a function $f$ is said to be invertible on an interval $I$

if there exists a function $f^{-1}$ such that for all $a\in I$ $f(a)=b$ if $f^{-1}(b)=a$

moreover,    $f(f^{-1}(x))=x$   $f^{-1}(f(x))=x$

the domain of $f^{-1}$ is $f(x)$ and the range of $f^{-1}$ is $I$.

to find $f^{-1}$ , given $y=f(x)$ we solve for $x$.



## Derivatives of Inverse function

we will look at the derivatives of inverse function via **the linear approximation**

in particular, the linear approximation of the inverse function at $f^{-1}$ at $y=b$ will be the **inverse** at the linear approximation of $f$  at $x=a$. ($f(a)=b$)



### Inverse function theorem(IFT)

if $f$ is invertible on $[c, d]$ and $f$ is differentiable on $(c, d)$,

then	if $a\in (c, d)$ and $f'(a) \neq 0$ , then $f^{-1}$ is differentiable at $b=f(a)$ and $(f^{-1} )'(b)=\frac{1}{f'(a)}=\frac{1}{f'(f^{-1}(b))}$



### Inverse trigonometric functions

for each trig function we can find an interval in which they are invertible

| Function                  | Domain                        | Range                                       |
| ------------------------- | ----------------------------- | ------------------------------------------- |
| $\arcsin x$               | $[-1,1]$                      | $[-\frac{\pi}{2},\frac{\pi}{2}]$            |
| $\arccos x$               | $[-1,1]$                      | $[0,\pi]$                                   |
| $\arctan x$               | $\mathbb{R}$                  | $(-\frac{\pi}{2},\frac{\pi}{2})$            |
| $\operatorname{arcsec} x$ | $(-\infty,-1] \or [1,\infty)$ | $[0,\frac{\pi}{2}) \or (\frac{\pi}{2},\pi]$ |
| $\operatorname{arccsc} x$ | $(-\infty,-1] \or [1,\infty)$ | $[-\frac{\pi}{2},0) \or (0, \frac{\pi}{2}]$ |
| $\operatorname{arccot} x$ | $\mathbb{R}$                  | $(0,\pi)$                                   |

common mistakes:

$ \arctan x = \tan^{-1}(x) \neq \frac{1}{\tan x}$



### Derivatives of inverse trig function

$\frac{d}{dx} \arcsin x = \frac{1}{\sqrt{1-x^2}}$

$\frac{d}{dx}\arccos x = \frac{-1}{\sqrt{1-x^2}}$

$\frac{d}{dx} \arctan x= \frac{1}{1+x^2}$

$\frac{d}{dx} \operatorname{arccot} x =\frac{-1}{1+x^2}$



#### Implicit differentiation

we have seen how to take the derivative of an explicitly defined function $y'=f(x)$

but sometimes we want to take the derivative of an implicitly defined function.

for example

$x^2 +y^2 =1$ or $x^3 +y^3 =6xy$

to find the slope $y'$ at some point $(x,y)$ we just assume $y$ is a function of $x$ and

take the derivative using the **chain Rule** (and any other rules needed)

$x^2 +y^2 = 1$ => $x^2 +(f(x))^2=1$

remember $y'$ is $y'$

note:

 if the function is not differentiable, then this gives a useless answer

ex:  $x^2 +y^2 =-1$ (no graph)  

$ 2x+2y'y=0$  $y'=-\frac{x}{y}$ (means nothing!)



##### Logarithmic differentiation

use properties of $ln$ and implicit differentiation

main purpose is to differentiable $y=(f(x))^{g(x)}$



# Local extrema

##### def: (local extrema)

a point $c$ is called **a local Max** of a function $f$ if

there exists an open interval $(a,b)$ containing $c$ such that $f(x)\le f(x)$ for all $x\in (a,b)$

a point $d$ is called **a local Min** of $f$ if

there exists an open interval $(a,b)$ containing $d$ such that $f(x)\ge f(d)$ for all $x\in (a,b)$



### Local Extrema Theorem(LET)(Fermat's Theorem)

if $c$ is a local Max or Min, and $f'(c)$ **exists**,  then $f'(c)=0$.

(a local extrema must occur at a critical point, but a critical point might not be either a local max or a local min)



#### def: (critical point)

let $f$ be a function and let $c$ be in the domain of $f$

if $f'(c)=0$ or $f'(c)$ DNE

then $c$ is called **a critical point** of $f$



### the Closed Interval Method

algorithm: let $f$ be a continuous function on $[a,b]$

step 1: evaluate $f$ at all critical points of $f$ inside $[a,b]$

step 2: evaluate $f(a),f(b)$

step 3: the largest value from steps 1, 2 tell you the **global Max**

​			the smallest value from steps 1, 2 tell you the **global Min**



### Rolle's Theorem

if $f$ is a function such that

1. $f$ is continuous on $[a,b]$
2. $f$ is differentiable on $(a,b)$
3. $f(a)=0=f(b)$

then there exists $c\in (a,b)$ such that $f'(c)=0$.

![](\pics\math\math137-2.jpg)

proof:

case1: if $f(x)=0$ for all $x$, then $f'(x)=0$ for all $x$, so take away $c\in (a,b)$

case2: if there exists $x \in(a,b)$ with $f(x_1)>0$

​			by the Extrema Value Theorem, there is a global Max, since the Max is not at $a$ or $b$ ,

​			by the Closed Interval Method, it must occur at a critical point

i.e.  some $c \in (a,b), f'(c)=0$

case3: Same as case 2



### the Mean Value Theorem (MVT)

if $f$ is a function, such that

1.  $f$ is continuous on $[a,b]$
2.  $f$ is differential on $(a,b)$

then there exists $c \in (a,b)$ such that $f'(c)=\frac{f(b)-f(a)}{b-a}$

![](\pics\math\math137-3.jpg)

proof:

let $h(x)=f(x)-f(a)-(\frac{f(b)-f(a)}{b-a})(x-a)$

since $f$ is continuous on $[a,b]$ , by the Continuity Theorem,  $h$ is also continuous on $[a,b]$

similarly, since $f$ is differentiable on $(a,b)$ by the Arithmetic Rules

for Derivatives $h$ is also differentiable on $(a,b)$

​	$h'(x)=f'(x)-0-\frac{f(b)-f(a)}{b-a}=f'(x)-\frac{f(b)-f(a)}{b-a}$

​	$h(a)=f(a)-f(a)-(\frac{f(b)-f(a)}{b-a})(a-a)=0$

​	$h(b)=f(b)-f(a)-(\frac{f(b)-f(a)}{b-a})(b-a)=0$

 by Rolle's theorem there exists $c\in (a,b)$ such that

​	$0=h'(c)=f'(c)-(\frac{f(b)-f(a)}{b-a})$

​	$\frac{f(b)-f(a)}{b-a}=f'(c)$

Notes:

1. MVT says that if $M$ is the slope at the secant line from $a$ to $b$  where $f$ has the same slope.($m$)
2. MVT is equivalent to Rolle's Theorem
3. our proof of MVT is an existence proof. so we cannot in general find $c$



## Applications of MVT

### Constant Function Theorem

if $f'(x)=0$ for all $x\in (a,b)$ and $f$ is continuous on $[a,b]$,

​	then there exists $k\in \mathbb{R}$ such that $f(x)=k$ for all $x\in (a,b)$

proof:

let $x_1, x_2 \in (a,b)$ with $x_1<x_2$ , since $f$ is differentiable on $(a,b)$, $f$ is differentiable on $[x_1, x_2]$

by Theorem , $f$ is continuous on $[x_1, x_2]$

by **MVT** there exists $c \in (x_1, x_2)$

​	such that

​		$0=f'(c)=\frac{f(x_2)-f(x_1)}{x_2 -x_1}$

​		$0=0(x_2 -x_1)=f(x_2)-f(x_1)$

​		$f(x_2)=f(x_2)$

​	say, $f(x_1)=k$.

so, $f(x)=k$ for all $x \in (a, b)$  



### Antiderivative Theorem

if $f'(x)=g'(x)$ for all $x$ in an interval $I$,

​	then there exists $c\in \mathbb{R}$ such that $f(x)=g(x)+c$

proof:

let $h(x)=f(x)-g(x)$.

by the Arithmetic Rules for derivatives $h(x)$ is also differentiable and $h'(x)=f'(x)-g'(x)=0$

since $h$ is differentiable, by theorem it is continuous.

so , by the Constant Function Theorem (from last class)

​	$h(x)=c$  $f(x)-g(x)=c$     $f(x)=g(x)+c$



### Increasing Function Theorem

Assume $f$ is differentiable on an interval $I$

let $x_1, x_2 \in I$ with $x_1 <x_2$

1. If $f'(x)>0$ for all $x\in I$, then $f$ is increasing , that is $f(x_1)<f(x_2)$
2. If $f'(x)\ge 0$ for all $x \in I$ , then $f$ is non-decreasing , that is $f(x_1)\le f(x_2)$
3. If $f'(x)<0$ for all $x\in I$, then $f$ is decreasing , that is $f(x_1)>f(x_2)$
4. If $f'(x)\le 0$ for all $x\in I$, then $f$ is non-increasing, that is $f(x_1)\ge f(x_2)$

proof of 1:

since $f$ is differentiable on $I$, it is differentiable on the subinterval $[x_1, x_2]$

by theorem , $f$ is also continuous on $[x_1,x_2]$

thus, by **MVT** there exists $c \in (x_1,x_2)$ such that

​		$0< f'(c)=\frac{f(x_2)-f(x_1)}{x_2 -x_1}$ since $x_2-x_1>0$

​		$0<f(x_2)-f(x_1)$

​		$f(x_1)<f(x_2)$



### Bounded Derivative Theorem (BDT)

If $f$ is continuous on $[a,b]$ and differentiable $(a,b)$ with $m\le f'(x)\le M$ for all $x\in (a,b)$

​	then $f(a)+m(x-a)\le f(x) \le f(a)+M(x-a)$ for all $x\in [a,b]$

Note:

if $f$ and $g$ are differentiable and $f'(x)>g'(x)$ for all $x>a$, what can we say about the <u>size</u> of $f(x)$

 compared to $g(x)$ for all $x>a$ ?

​	absolutely nothing!!!

​	if we also know $f(0)=0=g(0)$, and $a=0$.

#### Theorem

assume that $f$ and $g$ are continuous at $x=a$ and $f(a)=g(a)$

1. if $f$ and $g$ are both differentiable for $x>a$ and if $f'(x)\le g'(x)$ for all $x>a$, then $f(x)\ge g(x)$ for all $x>a$.
2. similarly, if $f'(x)\ge g'(x)$ for all $x>a$, then $f(x)\ge g(x)$ for all $x>a$
3. similarly for $>, <$



# Concavity and Inflection Points

#### Def: concavity

the graph of a function $f$ is called **concave up** on an interval $I$ if for **every** $a, b \in I$, the secant line between $(a,f(a))$ and $(b,f(b))$ is above the graph

the graph is **concave down** on $I$ if **every** secant line lies below the graph

### Theorem (derivative => concavity)

if $f^{\prime\prime}(x)>0$ for all $x$ in an interval $I$, then the graph of $f(x)$ is **concave up**

if $f^{\prime\prime}(x)<0$ for all $x\in I$, then the graph of $f(x)$ is **concave down** on $I$.

Note:

if the graph of $f$ is concave up on $I$, does that imply $f^{\prime\prime}(x)>0$ for all $x\in I$?

​	No, counterexample $f(x)=x^4, f'(x)=4x^3, f^{\prime\prime}(x)=12x^2$  when $f^{\prime\prime}(x)=0$

for short, we usually just say $f$ is concave up/down rather than the graph of $f$ is concave up/down



#### Def: inflection points

if $f$ is **continuous** at $x=c$, and $f$ changes concavity at $x=c$, then $x=c$ is called an inflection point of $f$.

make sure continuity !!!

fake changes:

![](\pics\math\math137-4.jpg)



### Theorem (property of inflection points)

if $f^{\prime\prime}(x)$ is **continuous** at $x=c$ and $x=c$ is an inflection point of $f$, then $f^{\prime\prime}(c)=0$



# Curve sketching

##### the 4 possibilities:

increasing and concave up

increasing and concave down

decreasing and concave up

decreasing and concave down

![](\pics\math\math137-5.jpg)



a local Max at $x=1$, local Min at $x=2$ and no inflection point? --- a straight line



### First derivative test: (always work!)

let $c$ be a critical point of $f$

1. if $f'$ changes sign from positive to negative at $x=c$ , then $c$ is a local Max of $f$.
2. if $f'$ changes sign from negative to positive at $x=c$, then $c$ is a local Min of $f$.

### Second derivative test:(when $f^{\prime\prime}(c)\neq $DNE)

let $c$ be a critical point of $f$ such that $f'(c)=0$

1. if $f^{\prime\prime}(c)>0$, then $c$ is a local Min of $f$.
2. if $f^{\prime\prime}(c)<0$, then $c$ is a local Max of $f$.
3. if $f^{\prime\prime}(c)=0$, then use the first derivative test



##### Steps:(curve sketching)

1. Domain
2. Intercepts (截距) $(x,0), (0,y)$
3. Horizontal or vertical asymptotes
<br>
   $\lim_{x\to \pm \infty} f(x)$  and $\lim_{x\to a^+ }f(x)=\pm \infty$
4. critical points and interval of increasing or decreasing
5. Intervals of concavity, inflection points
6. use info to sketch.



##### Using closed intervals for increase/decrease or concavity appropriately

writing a function that is increasing on $(a,b)$ and on $[b,c]$ , but is not increasing on $(a,c)$  ($a<b<c$)

![](\pics\math\math137-6.jpg)



# Taylor Polynomials

recall:

$L_a^f (x)= f(a)+f'(a)(x-a)$

##### Theorem

$\lim_{x\to a}\frac{f(x)-L_a^f(x)}{x-a}= 0$

proof: $\overset{LHR}{=} \lim_{x\to a}\frac{f'(x)-f'(a)}{1}$ (form $\frac{0}{0}$) $=0$

the **Error** in the **linear approximation theorem**:

​	if $&#124;f^{\prime\prime}(x)&#124;\le M$, $&#124;f(x)-L_a^f (x)&#124;\le \frac{M}{2} (x-a)^2$

we now want to get a better approximation by using a higher degree polynomial.

say $f(x)= C_0 +C_1(x-a)+C_2(x-a)^2$ $f(a)=C_0$

​	$f'(x)=C_1+2C_2(x-a)$

​	$f'(a)=C_1$

​	$f^{\prime\prime}(x)=2C_2$

​	$\frac{1}{2} f^{\prime\prime}(a)= C_2$

so, the second degree approximation is $T_{2,a} (x)= f(a)+f'(a)(x-a)+\frac{1}{2}f^{\prime\prime}(a)(x-a)^2$



#### Def: Taylor polynomial

let $f(x)$ be a function that is $n$ times differentiable.

we define the $n^{th}$ degree **Taylor polynomial** centered at $x=a$ by

​	$T_{n,a}(x)=f(a)+f'(a)(x-a)+\frac{f^{\prime\prime}(a)}{2!}(x-a)^2 +\frac{f^{(3)} (a)}{3!}(x-a)^3+\cdots +\frac{f^{(n)} (a)}{n!}(x-a)^n = \sum_{k=0}^n \frac{f^{(k)} (a)}{k!}(x-a)^k$



#### Def: Taylor remainder function

assume $f$ is $n$ times differentiable at $x=a$ ,

the function $R_{n,a}(x)=f(x)- T_{n,a}(x)$ is called the $n^{th}$ degree **Taylor remainder function** centered at $x=a$.

**Error** $= &#124;R_{n,a}(x)&#124;$

Taylor's Theorem assume $f$ is $n+1$ times differentiable on an interval $I$ containing $x=a$.

let $x\in I$, then there exists a point $c$ between $x$ and $a$ such that

​		$ f(x)-T_{n,a}(x)= R_{n,a}(x)= \frac{f^{(n+1)} (c)}{(n+1)!}(x-a)^{n+1}$

Notes:

1. it just says that $c$ exists. we do not have a general method for finding $c$.

2. Take $n=0$,  $f(x)-T_{0,a}(x)= \frac{f'(c)}{(0+1)!}(x-a)'$   

   $ \frac{f(x)-f(a)}{x-a} = f'(c)$  this is **MVT**

   so Taylor's Theorem is a *"higher order Mean Value Theorem"*

ex1:

let $f(x)=\sqrt{1+x}$

a. find $T_{2,0}(x)$

b. Approximate $\sqrt{2}$ using $T_{2,0} (x)$

c. Use Taylor 's Theorem to find an upper bound on the error in the approximation

sol:

a.  $f(x)=\sqrt{1+x}, f(0)=1, f'(x)= \frac{1}{2}(1+x)^{-\frac{1}{2}} f'(0)=\frac{1}{2}$

​	$f^{\prime\prime}(x)= -\frac{1}{4}(1+x)^{-3/2}f^{\prime\prime}(0)=-\frac{1}{4}$

​	$T_{2,0}(x)=f(0)+f'(a)(x-a)+\frac{f^{\prime\prime}(0)}{2!}(x-0)^2= 1+\frac{1}{2}x-\frac{1}{8}x^2$

b. $\sqrt{2} = f(1) \approx T_{2,0}(1)= 1+\frac{1}{2}(1)-\frac{1}{8}(1)^2= 1.375$

c. by Taylor's theorem, we need the ${n+1}^{th}$ derivative

​	i.e. 3rd derivative

​	$f^{(3)} (x)= \frac{3}{8}(1+x)^\frac{-5}{2}$   $R_{2,0} (x)= \frac{f^{(3)} (c)}{3!}(x-0)^3$ where $c\in (0,1)$

​	observe $&#124;f^{(3)} (c)&#124;= &#124;\frac{3}{8}\cdot \frac{1}{(1+c)^{\frac{5}{2}}}&#124;< \frac{3}{8}$  $&#124;error&#124;= &#124;R_{2,0}(x)&#124;= &#124;\frac{f^{(3)} (c)}{3!}x^3&#124;< \frac{3}{8}\cdot \frac{1}{6}&#124;x&#124;^3< \frac{1}{16}$

ex2:

let $f(x)=\frac{1}{x}$

 a. Find $T_{2,1} (x)$

b. Approximate $\frac{1}{1\cdot 1}$ by using $T_{2,1} (x)$

c. Use Taylor's Theorem to find an upper bound for the error in the approximation

sol:

a. $f(x)= x^{-1}, f(1)=1, f'(x)=-x^{-2}, f'(1)= -1, f^{\prime\prime}(x)= 2x^{-3}, f^{\prime\prime}(1)= 2$

​	$T_{2,1}(x)= f(1)+f'(1)(x-1) +\frac{f^{\prime\prime}(1)}{2!}(x-1)^2= 1-(x-1)+(x+1)^2$

b. $\frac{1}{1.1}= f(1.1) \approx T_{2,1} (1.1)=0.91$

c. $f^{(3)} (x)= -6x^{-4}<1$

​	$&#124;error&#124;= &#124;R_{2,1}(x)&#124;= &#124;\frac{f^{(3)} (c)}{3!}(x-1)^3&#124; <\frac{6}{6}(0.1)^3=0.001$

​	$c\in (1, 1.1)$



#### Application of Taylor's Theorem

Example:  Evaluate $\lim_{x\to 0}\frac{e^x -1-x}{x^2}$

sol: let $f(x)=e^x$ . On the interval $(-1, 1)$ , we get by **Taylor's Theorem** that there exists $c\in (-1, 1)$ such that

​		$e^x = 1+x+\frac{x^2}{2!} +\frac{f^{(3)} (c)x^3}{3!}$  so, $\lim_{x\to 0} \frac{e^x -1-x}{x^2}= \lim_{x\to 0} \frac{\frac{x^2 }{2!}+\frac{f^{(3)} (c)}{3!}x^3 }{x^2 }= \lim_{x\to 0}(\frac{1}{2}+\frac{f^{(3)} (c)}{3!})= \frac{1}{2}$



Example: Evaluate $\lim_{x\to 0} \frac{ {e^x}^4 -\cos x^2 }{x^4 }$

sol: we can show

${e^x}^4 = 1+x^4 +\underbrace{\frac{f^{(8)} (c)}{8!} x^8}$  

​							$T_{7,0}(0)$ and apply Taylor's Theorem

(aside: $f(x)=e^x, T_{2,0}(x)=1+x+\frac{x^2}{2!}$ )

we can prove if $f(x)={e^x}^4$ then  $T_{2,0} (x)=1+x+\frac{x^2}{2!}$

$\cos x^2= 1- \frac{x^4 }{2!}+\frac{f^{(8)} (c)x^8}{8!}$

then   $= \lim_{x\to 0} \frac{ {e^x}^4 -\cos x^2} {x^4} = \lim_{x\to 0} \frac{x^4 +\frac{f^{(8)} (c)x^8 }{8!}-\frac{x^4 }{2!}+\frac{g^{(8)} (c)x^8 }{8!}}{x^4}= \lim_{x\to 0} 1+f^{(8)} (c)x^4 +\frac{1}{2}-g^{(8)} (c)x^4 = \frac{3}{2}$
