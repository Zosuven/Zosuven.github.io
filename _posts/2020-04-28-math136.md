---
layout: post
title: 'MATH 136'
date: 2020-04-28
author: Zosuven
color: rgb(230,230,250)
cover: '/pics/math/cover-math136.jpg'
tags: notes math
---
{% include toc.html %}

# Chapter 1 Vectors in Euclidean Space

## large space $\mathbb{R}^n$

### Def $\mathbb{R}^n $
the set $\mathbb{R}^n $ is defined by

$\mathbb{R}^n =\begin{Bmatrix} \vec{x} = \begin{bmatrix} x_1&#92;&#92;\vdots&#92;&#92;x_n \end{bmatrix} &#124; x_1, \dots, x_n \in \mathbb{R}  \end{Bmatrix}$

two vectors $\vec{x}=\begin{bmatrix}x_1&#92;&#92;\vdots&#92;&#92;x_n\end{bmatrix}$ and $\vec{y}=\begin{bmatrix}y_1&#92;&#92;\vdots&#92;&#92;y_n\end{bmatrix}$ in $\mathbb{R}^n$ are said to be **equal**
if $x_i=y_i$ for $1\le i\le n$.


### Def Vector Addition & Scalar Multiplication

ps: vector rule for addition is sometimes called **parallelogram rule for additon**

坐标加的话就正好形成一个平行四边形

let $\vec{x}= \begin{bmatrix}x_1&#92;&#92;\vdots&#92;&#92;x_n\end{bmatrix}$ , $\vec{y}= \begin{bmatrix}y_1&#92;&#92;\vdots&#92;&#92;y_n\end{bmatrix} \in \mathbb{R}^n$ and let $c\in \mathbb{R}$ ($c$ is a **scalar**)

we define **addition** $\vec{x} +\vec{y}$, and **scalar multiplication** $c\vec{x}$, by

$\vec{x}+\vec{y}= \begin{bmatrix}
x_1+y_1&#92;&#92;\vdots&#92;&#92;x_n +y_n \end{bmatrix}$

$c\vec{x}=\begin{bmatrix}
cx_1&#92;&#92;\vdots&#92;&#92;cx_n\end{bmatrix}$


### Def linear combination

for $\vec{v_1}, \dots, \vec{v_k} \in \mathbb{R}^n $ and $c_1, \dots, c_k \in \mathbb{R}$ we call sum

$c_1\vec{v_1}+c_2\vec{v_2}+ \dots + c_k\vec{v_k}$

a **linear combination** of $\vec{v_1}, \dots, \vec{v_k}$

### Theorem1.1.1 (ten rules)

if $\vec{x}, \vec{y}, \vec{w} \in \mathbb{R}^n$ , and $c, d \in \mathbb{R}$, then

V1 $\vec{x}+\vec{y} \in \mathbb{R}^n$

V2 $(\vec{x}+\vec{y})+\vec{w}= \vec{x}+(\vec{y}+\vec{w})$

V3 $\vec{x}+\vec{y}=\vec{y}+\vec{x}$

V4 there exists a vector $\vec{0}$ such that $\vec{x}+\vec{0}=\vec{x}$ for all $\vec{x} \in \mathbb{R}^n$

V5 for every $\vec{x}\in \mathbb{R}^n$ there exists $(-\vec{x})\in \mathbb{R}^n$ such that $\vec{x}+(-\vec{x})=\vec{0}$

V6 $c\vec{x}\in \mathbb{R}^n$

V7 $c(d\vec{x})=(cd)\vec{x}$

V8 $(c+d)\vec{x}=c\vec{x}+d\vec{x}$

V9 $c(\vec{x}+\vec{y})=c\vec{x}+c\vec{y}$

V10 $1\vec{x}=\vec{x}$

conclusion:

V2, V3, V7, V8, V9 ,V10 are **operations**

V1, V4, V5, V6 are **relationship between operations and** $\mathbb{R}^n$

V4 says: exists **zero vector** $\vec{0}=\begin{bmatrix}0&#92;&#92;\vdots&#92;&#92;0\end{bmatrix}$

V5 says: exists **additive inverse** of $\vec{x}$ is $-\vec{x}=\begin{bmatrix}-x_1&#92;&#92;\vdots&#92;&#92;-x_n\end{bmatrix}$

V1 and V6 show: $\mathbb{R}^n$ is **closed under linear combination**

### Span

first:
**vector equation** is like: $\vec{x}=c\begin{Bmatrix}1&#92;&#92;1\end{Bmatrix}$

in it , $\vec{v}=\begin{Bmatrix}1&#92;&#92;1\end{Bmatrix}$ is **direction vector**

def of **span**

let $\beta = \begin{Bmatrix}\vec{v_1}, \dots,\vec{v_k}\end{Bmatrix}$ be a set of vector in $\mathbb{R}^n$.
we define the **span** of $\beta$ by

$span \beta =\begin{Bmatrix}c_1\vec{v_1}+c_2\vec{c_2}+\dots+c_k\vec{v_k} &#124; c_1, \dots, c_k \in \mathbb{R}\end{Bmatrix}$

we say that the set $\operatorname{Span} \beta$ is **spanned** by $\beta$ and that $\beta$ is a **spanning set** for $\operatorname{Span} \beta$.

#### Theorem 1.2.1

let $\vec{v_1}, \dots, \vec{v_k} \in \mathbb{R}^n$. Some vector $\vec{v_i}$, $1\le i\le k$, can be written as a linear combination of $\vec{v_1}, \dots, \vec{v_{i-1}}, \vec{v_{i+1}}, \dots, \vec{v_k}$ if and only if

$\operatorname{Span} \begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix}= \operatorname{Span} \begin{Bmatrix}\vec{v_1}, \dots,\vec{v_{i-1}}, \vec{v_{i+1}}, \dots, \vec{v_k} \end{Bmatrix}$

### Linearly Dependent & Independent

a set of vectors $\begin{Bmatrix}\vec{v_1},\dots ,\vec{v_k}\end{Bmatrix} \in \mathbb{R}^n$ is said to

**linearly dependent** if there exist coefficients $c_1, \dots, c_k$ not all zero such that

$\vec{0}=c_1\vec{v_1}+\dots+c_k\vec{c_k}$

**linearly Independent** if the only solution to

$\vec{0}=c_1\vec{v_1}+\dots+c_k\vec{v_k}$ is $c_1=c_2=\dots=c_k=0$ (called the **trivial solution**)

#### Theorem 1.2.2

a set of vectors $\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix} \in \mathbb{R}^n$ is linearly dependent if and only if
$\vec{v_i}\in \operatorname{Span} \begin{Bmatrix}\vec{v_1}, \dots, \vec{v_{i-1}}, \vec{v_{i+1}}, \dots, \vec{v_k}\end{Bmatrix}$ for some $i$, $1\le i \le k$.

#### Theorem 1.2.3

if a set of vectors $\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix}$ contains the zero vector, then it is linearly dependent


### Basis

let $S$ be a subset of $\mathbb{R}^n$

if $\begin{Bmatrix}\vec{v_1}, \dots,\vec{v_k}\end{Bmatrix}$ is a linearly independent set of vectors in $\mathbb{R}^n$ such that $S=\operatorname{Span} \begin{Bmatrix}\vec{v_1},\dots, \vec{v_k}\end{Bmatrix}$, then the set $\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix}$ is called a **basis** for $S$.

we define a basis for the set $\begin{Bmatrix}\vec{0}\end{Bmatrix}$ to be the empty set.

#### standard basis

in $\mathbb{R}^n$, let $\vec{e_i}$ represents the vector whose i-th component is $1$ and all other components are $0$. the set $\begin{Bmatrix}\vec{e_i}, \dots, \vec{e_n}\end{Bmatrix}$ is called the **standard basis** for $\mathbb{R}^n$

#### Theorem 1.2.4 (basis for a set)

if $\beta =\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix}$ is a basis for a subset $S$ of $\mathbb{R}^n$, then every vector $\vec{x} \in S$ can be written as a unique linear combination of the vector in $\beta$.


### Surfaces in Higher Dimensions

#### Def **line** in $\mathbb{R}^n$

let $\vec{v}, \vec{b} \in \mathbb{R}^n$ with $\vec{v} \neq \vec{0}$.

we call the set with vector equation $\vec{x}=c_1\vec{v}+\vec{b}$, $c_1\in \mathbb{R}$ a **line** in $\mathbb{R}^n$
through $\vec{b}$.

#### Def **Plane** in $\mathbb{R}^n$

let $\vec{v_1}, \vec{v_2}, \vec{b}\in \mathbb{R}^n$ with $\begin{Bmatrix}\vec{v_1}, \vec{v_2}\end{Bmatrix}$ being a linearly independent set.

we call the set with vector equation $\vec{x}=c_1\vec{v_1}+c_2\vec{v_2}+\vec{b}, c_1, c_2 \in \mathbb{R}$ a **plane** in $\mathbb{R}^n$ through $\vec{b}$.

#### Def **Hyperplane** in $\mathbb{R}^n$

let $\vec{v_1}, \dots, \vec{v_{n-1}}, \vec{b}\in \mathbb{R}^n$ with $\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_{n-1}}\end{Bmatrix}$ being linearly independent.

the set with vector equation $\vec{x}=c_1\vec{v_1}+\dots+c_{n-1}\vec{v_{n-1}}+\vec{b}, c_1, \dots, c_{n-1}\in \mathbb{R}$ is called a **Hyperplane** in $\mathbb{R}^n$ through $\mathbb{b}$.

#### Def k-Flat in $\mathbb{R}^n$

let $\vec{v_1},\dots, \vec{v_k}, \vec{b}\in \mathbb{R}^n$ with $\begin{Bmatrix}\vec{v_1},\dots,\vec{v_k}\end{Bmatrix}$ linearly independent.

we call the set with vector equation $\vec{x}=c_1\vec{v_1}+\dots+c_k\vec{v_k}+\vec{b}, c_1, \dots, c_k \in \mathbb{R}$ a k-flat in $\mathbb{R}^n$ through $\vec{b}$.

**a point** in $\mathbb{R}^n$ is a **0-flat**

**a line** in $\mathbb{R}^n$ is a **1-flat**

**a hyperplane** in $\mathbb{R}^n$ is an **(n-1)-flat**


## Subspaces

that is a "Space"(non-empty) which is a subset of the larger space $\mathbb{R}^n$.

### **Def** of **subspaces**:

a subset $\mathbb{S}$ of $\mathbb{R}^n$ is called a **subspace of** $\mathbb{R}$ if for every $\vec{x}, \vec{y}, \vec{w}\in \mathbb{S}$ and, $c,d\in \mathbb{R}$ we have

S1 $\vec{x}+\vec{y}\in \mathbb{S}$

S2 $(\vec{x}+\vec{y})+\vec{w}=\vec{x}+(\vec{y}+\vec{w})$

S3 $\vec{x}+\vec{y}=\vec{y}+\vec{x}$

S4 There exists a vector $\vec{0} \in \mathbb{S}$

S5 for every $\vec{x}\in \mathbb{S}$ there exists $(-\vec{x})\in \mathbb{S}$ such that $\vec{x}+(-\vec{x})=\vec{0}$

S6 $c\vec{x}\in S$

S7 $c(d\vec{x})=(cd)\vec{x}$

S8 $(c+d)\vec{x}=c\vec{x}+d\vec{x}$

S9 $c(\vec{x}+\vec{y})=c\vec{x}+c\vec{y}$

S10 $1\vec{x}=\vec{x}$

note:

S2, S3, S7, S8, S9, S10 are only about the operations of additon and scalar multiplication

if S1 and S6 hold, then for any $\vec{v} \in \mathbb{S}$, we have $\vec{0}=0\vec{v}\in \mathbb{S}$ and $(-\vec{v})=(-1)\vec{v}\in \mathbb{S}$, so S4 and S5 hold

#### Theorem 1.3.1 (Subspace Test)

let $\mathbb{S}$ be a non-empty subset of $\mathbb{R}^n$.

if $\vec{x}+\vec{y}\in \mathbb{S}$ and $c\vec{x}\in \mathbb{S}$ for all $\vec{x}, \vec{y}\in \mathbb{S}$ and $c\in \mathbb{R}$, then $\mathbb{S}$ is a subset of $\mathbb{R}^n$

#### Theorem 1.3.2

if $\vec{v_1}, \dots, \vec{v_k}\in \mathbb{R}^n$, then $\mathbb{S}=\operatorname{Span} \begin{Bmatrix}\vec{v_1},\dots, \vec{v_k}\end{Bmatrix}$ is a subspace of $\mathbb{R}^n$

implies that if a subset $\mathbb{S}$ of $\mathbb{R}^n$ has a basis, then $\mathbb{S}$ must be a subspace of $\mathbb{R}^n$

### Bases of Subspace

the procedure for finding a basis of a given subspace:

note:

1. use Theorem 1.2.1 直到 get a <span style="color:red">**linearly independent**</span> spanning set for $\mathbb{S}$

2. for bases of $\mathbb{R}^n$, every subspace of $\mathbb{R}^n$, excluding the trivial subspace $\begin{Bmatrix}\vec{0}\end{Bmatrix}$, will have infinitely many bases.

questions:

type 1:

prove that a set $\beta$ spans a subspace $\mathbb{S}$ of $\mathbb{R}^n$ ($\operatorname{Span} \beta = \mathbb{S}$)

it is to prove that $\operatorname{Span} \beta \subseteq \mathbb{S}$ and $\mathbb{S}\subseteq\operatorname{Span}\beta$

note that if all of the vectors in $\beta$ are in $\mathbb{S}$, (this step is **trivial**, and often *omitted*)

then we must have $\operatorname{Span}\beta \subseteq \mathbb{S}$,

since $\mathbb{S}$ is **closed under linear combination**

type 2:

find a basis of a given subspace

1. find the general form of a given subspace

eg. a linear equation $=c_1\vec{v_1}+\dots+c_k\vec{v_k}$ note that the $c_1,\dots, c_k\in \mathbb{R}$ are uncertain number, and the $\vec{v_1}, \dots, \vec{v_k}$ has the specific value

2. judge if the a set of vector $\vec{v_1}, \dots, \vec{v_k}$ are linearly independent: if not, making them to linearly independent

3. this set of vectors are the bases


## Dot Product

also called **standard inner product** on $\mathbb{R}^n$, or **scalar product**

recall the dot product in $\mathbb{R}^2$ and $\mathbb{R}^3$ is defined by

$\begin{bmatrix}x_1&#92;&#92;x_2\end{bmatrix}\cdot\begin{bmatrix}y_1&#92;&#92;y_2\end{bmatrix}=x_1 y_1+x_2 y_2$

$\begin{bmatrix}x_1&#92;&#92;x_2&#92;&#92;x_3\end{bmatrix}\cdot\begin{bmatrix}y_1&#92;&#92;y_2&#92;&#92;y_3\end{bmatrix}=x_1 y_1+x_2 y_2+x_3 y_3$

def: Dot Product

let $\vec{x}=\begin{bmatrix}x_1&#92;&#92;\vdots&#92;&#92;x_n\end{bmatrix}$ and $\vec{y}=\begin{bmatrix}y_1&#92;&#92;\vdots&#92;&#92;y_n\end{bmatrix}$ be vectors in $\mathbb{R}^n$.

the **dot product** of $\vec{x}$ and $\vec{y}$ is $\vec{x}\cdot\vec{y}=\begin{bmatrix}x_1&#92;&#92;\vdots&#92;&#92;x_n\end{bmatrix}\cdot\begin{bmatrix}y_1&#92;&#92;\vdots&#92;&#92;y_n\end{bmatrix}=x_1 y_1+\cdots+x_n y_n=\sum_{i=1}^n x_i y_i$

when use?

1. calculating the **length** or **norm** of a vector $\&#124;\vec{v}\&#124;=\sqrt{\vec{v}\cdot\vec{v}}$

2. determining if two vwctors are orthogonal: if are: $\vec{x}\cdot\vec{y}=0$

3. determining the angle between two vectors (theorem 1.4.1)


### Theorem 1.4.1

if $\vec{x}, \vec{y} \in \mathbb{R}^2$ and $\theta$ is an angle between $\vec{x}$ and $\vec{y}$, then $\vec{x}\cdot\vec{y}=\&#124;\vec{x}\&#124; \&#124;\vec{y}\&#124;\cos \theta$

### Theorem 1.4.2

if $\vec{x}, \vec{y}, \vec{z}\in \mathbb{R}^n$ and $s, t\in \mathbb{R}$, then

1. $\vec{x}\cdot\vec{x}\ge 0$, and $\vec{x}\cdot\vec{x}=0$ if and only if $\vec{x}=\vec{0}$

2. $\vec{x}\cdot\vec{y}=\vec{y}\cdot\vec{x}$

3. $\vec{x}\cdot(s\vec{y}+t\vec{z})=s(\vec{x}\cdot\vec{y})+t(\vec{x}\cdot\vec{z})$

### Unit Vector

a vector $\vec{x}\in \mathbb{R}^n$ such that $\&#124;\vec{x}\&#124;=1$ is called a **Unit vector**

### Theorem 1.4.3

if $\vec{x},\vec{y}\in \mathbb{R}^n$ and $c\in \mathbb{R}$, then

1. $\&#124;\vec{x}\&#124;\ge 0$, and $\&#124;\vec{x}\&#124;=\vec{0}$ if and only if $\vec{x}=\vec{0}$

2. $\&#124;c\vec{x}\&#124;=&#124;c&#124; \&#124;\vec{x}\&#124;$

3. $&#124;\vec{x}\cdot\vec{y}&#124;\le\&#124;\vec{x}\&#124; \&#124;\vec{y}\&#124;$ (Cauchy-Schwarz_Bunyakovski Inequality)

4. $\&#124;\vec{x}+\vec{y}\&#124;\le \&#124;\vec{x}\&#124; +\&#124;\vec{y}\&#124;$ (triangle Inequality)

### Def: **Angle in** $\mathbb{R}^n$

let $\vec{x}, \vec{y}\in \mathbb{R}^n$. we define an **angle** between $\vec{x}$ and $\vec{y}$ to be an angle $\theta$ such that $\vec{x}\cdot\vec{y}=\&#124;\vec{x}\&#124; \&#124;\vec{y}\&#124; \cos \theta$

### Def: **orthogonal**

let $\vec{x}, \vec{y}\in \mathbb{R}^n$. if $\vec{x}\cdot\vec{y}=0$, then $\vec{x}$ and $\vec{y}$ are said are to be **orthogonal**

### Theorem  1.4.4

the zero vector $\vec{0}\in \mathbb{R}^n$ is orthogonal to **every** vector $\vec{x}\in \mathbb{R}^n$.

### Def: **orthogonal set**

a set of vectors $\begin{Bmatrix}\vec{v_1},\dots,\vec{v_k}\end{Bmatrix}$ in $\mathbb{R}^n$ is called an **orthogonal set** if $\vec{v_i}\cdot\vec{v_j}=0$ for all $i\neq j$.

note: a set of unit vectors is an orthogonal set.

## Cross Product

cross product only works when $\mathbb{R}^3$

let $\vec{v}, \vec{w}\in \mathbb{R}^3$.

the **cross product** of $\vec{v}=\begin{bmatrix}v_1&#92;&#92;v_2&#92;&#92;v_3\end{bmatrix}$ and $\vec{w}=\begin{bmatrix}w_1&#92;&#92;w_2&#92;&#92;w_3\end{bmatrix}$ is said to be

$\vec{v}\times\vec{w}=\begin{bmatrix}v_2w_3-v_3w_2&#92;&#92;v_3w_1-v_1w_3&#92;&#92;v_1w_2-v_2w_1\end{bmatrix}$

### Theorem 1.4.5

Suppose that $\vec{v}, \vec{w}, \vec{x}\in \mathbb{R}^3$ and $c\in \mathbb{R}$.

1.if $\vec{n}=\vec{v}\times\vec{w}$, then for any $\vec{y}\in \operatorname{Span}\begin{Bmatrix}\vec{v},\vec{w}\end{Bmatrix}$ we have $\vec{y}\cdot\vec{n}=0$

($\vec{n}$ actually is the orthogonal vector of the plane produced by the $\vec{v}\times\vec{w}$)


2.$\vec{v}\times\vec{w}=-\vec{w}\times\vec{v}$


3.$\vec{v}\times\vec{v}=\vec{0}$


4.$\vec{v}\times\vec{w}=\vec{0}$ if and only if either $\vec{v}=\vec{0}$ or $\vec{w}$ is a scalar muitiple of $\vec{v}$


5.$\vec{v}\times(\vec{w}+\vec{x})=\vec{v}\times\vec{w}+\vec{v}\times\vec{x}$


6.$(c\vec{v})\times(\vec{w})=c(\vec{v}\times\vec{w})$


7.$\&#124;\vec{v}\times\vec{w}\&#124;=\&#124;\vec{v}\&#124; \&#124;\vec{w}\&#124; &#124;\sin \theta &#124;$ where $\theta$ is the angle between $\vec{v}$ and $\vec{w}$

## Scalar Equation of a Plane

### Theorem 1.4.6

let $\vec{v}, \vec{w}, \vec{b}\in \mathbb{R}^3$ with $\begin{Bmatrix}\vec{v}, \vec{w}\end{Bmatrix}$ being linearly independent and

let $P$ be a plane in $\mathbb{R}^3$ with vector equation $\vec{x}=s\vec{v}+t\vec{w}+\vec{b}, s, t\in \mathbb{R}$

if $\vec{n}=\vec{v}\times\vec{w}$, then an equation for the plane is $(\vec{x}-\vec{b})\cdot\vec{n}=0$

### Def: Normal Vector & Scalar equation

let $P$ be a plane in $\mathbb{R}^3$ that passes through the point $B(b_1, b_2, b_3)$. If $\vec{n}=\begin{bmatrix}n_1&#92;&#92;n_2&#92;&#92;n_3\end{bmatrix}\in \mathbb{R}^3$ is a vector such that

(the form: $\vec{x}\cdot\vec{n}=\vec{b}\cdot\vec{n}$)

$n_1x_1+n_2x_2+n_3x_3=b_1n_1+b_2n_2+b_3n_3$ (1)

is an equation for $P$, then $\vec{n}$ is called a **normal vector** for $P$. we call equation (1) a **scalar equation** of $P$

## Scalar Equation of Hyperplane

If $\begin{Bmatrix}\vec{v_1}, \dots,\vec{v_{m-1}}\end{Bmatrix}$ is a linearly independent set of vectors in $\mathbb{R}^m , m\ge 2$ , and $\vec{b}\in \mathbb{R}^m$,

then , $\vec{x}=c_1\vec{v_1}+\cdots+c_{m-1}\vec{v_{m-1}}+\vec{b}$ is a vector equation of a hyperplane in $\mathbb{R}^m$.

if there exists a non-zero vector $\vec{n}=\begin{bmatrix}n_1&#92;&#92;\vdots&#92;&#92;n_m\end{bmatrix}\in \mathbb{R}^m$ which is orthogonal to each of $\vec{v_1}, \dots, \vec{v_{m-1}}$,

then (with the form $\vec{x}\cdot\vec{n}=\vec{b}\cdot\vec{n}$) the **scalar equation** is

$n_1x_1+\cdots+n_mx_m=n_1b_1+\cdots+n_mb_m$

## Projections

### Def: **projection** onto a **Line** in $\mathbb{R}^n$

let $\vec{u}, \vec{v}\in \mathbb{R}^n$ with $\vec{v}\neq \vec{0}$. the **projection** of $\vec{u}$ onto $\vec{v}$ is defined by

$\operatorname{proj}_{\vec{v}} (\vec{u})=\frac{\vec{u}\cdot\vec{v}}{(\&#124;\vec{v}\&#124;)^2} \vec{v}$

### Def: **Perpendicular** of a Projection in $\mathbb{R}^n$

let $\vec{u}, \vec{v} \in \mathbb{R}^n$ with $\vec{v}\neq \vec{0}$. the **Perpendicular** of $\vec{u}$ into $\vec{v}$ is defined by

$\operatorname{perp}_ {\vec{v}} (\vec{u})=\vec{u}- \operatorname{proj}_ {\vec{v}}(\vec{u})$

### Def: **Projection** and **Perpendicular** onto a **Plane** in $\mathbb{R}^3$

let $P$ be a plane in $\mathbb{R}^3$ that passes through the origin and has normal vector $\vec{n}$,

the **projection** of $\vec{x}\in \mathbb{R}^3$ onto $P$ is defined by  $\operatorname{proj}_ {P} (\vec{x})=\operatorname{perp}_ {\vec{n}} (\vec{x})$

the **perpendicular** of $\vec{x}$ onto $P$ is defined by $\operatorname{perp}_ {P} (\vec{x})=\operatorname{proj}_ {\vec{n}} (\vec{x})$

# Chapter 2

## System of Linear Equation

### Def: linear equation

an equation in $n$ variables (unknowns) $x_1, \dots, x_n$ that can be written in the form $a_1x_1+\cdots+a_nx_n=b$

where $a_1, \dots, a_n, b$ are constants is called a **linear equation**.

the constants $a_i$ are called the **coefficients** of the equation and $b$ is called the **right-hand side**


### Def: System of linear Equations

a set of $m$ linear equations in the same variables $x_1, \dots, x_n$ is called a **System of** $m$ **linear equations in** $n$ **variables**

a general system of $m$ linear equations in $n$ variables has the form:

![](\pics\math\math136-1.jpg)

observe that the coefficient $a_{ij}$ represents the coefficient of $x_j$ in the $i-th$ equation

### Def: Solution of a System & Solution Set

a vector $\vec{s}=\begin{bmatrix}s_1&#92;&#92;\vdots&#92;&#92;s_n\end{bmatrix} \in \mathbb{R}^n$ is called a **solution** of a system of $m$ linear equations in $n$ variables

if all $m$ equations are satisfied  when we set $x_i=s_i$ for $1\le i\le n$.

the **set** of all solutions of a system of linear equations is called the **solution set** of the system.

### Def: Consistent & Inconsistent

if a system of linear equations has at least one solution, then it is said to be **consistent**

otherwise, it is said to be **inconsistent**.

<span style="color:red">✮</span> **Geometrically**, a system of $m$ system of $m$ linear equations in $n$ variables represents $m$ hyperplanes in $\mathbb{R}^n$

thus, a solution of the system is a vector in $\mathbb{R}^n$ which lies on all $m$ hyperplanes.

the system is inconsistent if all $m$ hyperplanes do not share a point of intersection (e.g. hyperplanes are parallel)

### Theorem 2.1.1

if the system of linear equations

![](\pics\math\math136-1.jpg)

has two distinct solutions $\vec{s}=\begin{bmatrix}s_1&#92;&#92;\vdots&#92;&#92;s_n\end{bmatrix}$ and $\vec{t}=\begin{bmatrix}t_1&#92;&#92;\vdots&#92;&#92;t_n\end{bmatrix}$,

then $\vec{x}=\vec{s} +c(\vec{s}-\vec{t})$ is a distinct solution for each $c\in \mathbb{R}$

note that the sign "$-$" is the key point

it shows that the solution set of a system of $m$ linear equations in $n$ variables

must either **be empty** , contain **exactly one vector**, or have **infinitely many vectors**


## Solving Systems of Linear Equations

### Def: Equivalent

two systems of equations which have the same solution set are said to be **equivalent**.

### Def: Augmented Matrix & Coefficient Matrix

the **coefficient matrix** for the system of linear equations

![](\pics\math\math136-1.jpg)

is the rectangular array

$\begin{bmatrix}a_{11}&a_{12}&\cdots&a_{1n}&#92;&#92;a_{21}&a_{22}&\cdots&a_{2n}&#92;&#92;\vdots&\vdots&\vdots&\vdots&#92;&#92;a_{m1}&a_{m2}&\cdots&a_{mn}\end{bmatrix}$

the **augmented matrix** of the system is

$\left[\begin{array}{cccc&#124;c} a_{11}&a_{12}&\cdots&a_{1n}&b_1&#92;&#92;a_{21}&a_{22}&\cdots&a_{2n}&b_2&#92;&#92;\vdots&\vdots&\vdots&\vdots&\vdots&#92;&#92;a_{m1}x_1&a_{m2}&\cdots&a_{mn}&b_m \end{array}\right]$

it can be written as the form:

$x_1\begin{bmatrix}a_{11}&#92;&#92;\vdots&#92;&#92;a_{m1}\end{bmatrix}+\cdots+x_n\begin{bmatrix}a_{1n}&#92;&#92;\vdots&#92;&#92;a_{mn}\end{bmatrix}$

the coefficient matrix of the system  are the vectors $\vec{a_j}=\begin{bmatrix}a_{1j}&#92;&#92;\vdots&#92;&#92;a_{mj}\end{bmatrix}$ for $1\le j\le n$

thus, we can denote the coefficient matrix of the system by $A=\begin{bmatrix}\vec{a_1}\cdots\vec{a_n}&#124;\vec{b}\end{bmatrix}$
or sometimes simply as $\begin{bmatrix}A&#124;\vec{b}\end{bmatrix}$

### Def: Elementary Row Operations (EROs)

the Three **elementary row operations** are:

1. multiplying a row by a non-zero scalar

2. adding a multiple of one row to another

3. swapping two rows

using those 3 operations we can transform $A$ into $B$, and we can also transform $B$ into $A$

### Def: Row Equivalent

two matrices $A$ and $B$ are said to be **row equivalent** if there exists a sequence of elementary row operations that transform $A$ into $B$. we write $A$ ~ $B$

### Theorem 2.2.1

if the augmented matrices $\begin{bmatrix}A_1&#124;\vec{b_1}\end{bmatrix}$ and $\begin{bmatrix}A&#124;\vec{b}\end{bmatrix}$ are row equivalent,

then the systems of linear equations associated with each system are equivalent.

### Def: Reduced Row Echelon Form (RREF)

a matrix $R$ is said to be in **reduced row echelon form** if:

1. all rows containing a non-zero entry are above rows which only contains zeros

2. the first non-zero entry in each non-zero row is 1, called a **leading one**

3. the leading one in each non-zero is to the right of the leading one in any row above it

4. a leading one is the non-zero entry in its column.

if $A$ is row equivalent to a matrix $R$ in **RREF**, then we say that $R$ is the **reduced row echelon form** of $A$

### Theorem 2.2.2

if $A$ is a matrix, then $A$ has a unique reduced row echelon form $R$.

### Def: Free Variable

let $R$ be the **RREF** of a coefficient matrix of a consistent system of linear equations.

if the $j-th$ column of $R$ does not contain a leading one,

then we call $x_j$ a **free variable** of the system of linear equations.

### Algorithm

to solve a system of linear equations:

1. write the augmented matrix for the system.

2. use elementary row operations to row reduce the augmented matrix into **RREF**

3. write the system of linear equations corresponding to the **RREF**

4. if the system contains an equation of the form $0=1$, the system is **inconsistent**.

5. otherwise, move each free variable(if any) to the right hand side of each equation and assign each free variable a parameter

6. determine the solution set by using vector operations to write the system as a linear combination of vectors

## Homogeneous Systems

a system of linear equations is said to be a **homogeneous system**

if the right-hand side only contains zeros.

That is, it has the form $\begin{bmatrix}A&#124;\vec{0}\end{bmatrix}$

features:

1. homogeneous system is always consistent

2. zero vector $\vec{0}$ is always a solution of a homogeneous system.

3. no free variable in homogeneous system means the only solution of variables is $0$, and it means **linearly independent**

### Theorem 2.2.3

the solution set of a homogeneous system of $m$ linear equations in $n$ variables is a subspace of $\mathbb{R}^n$

because:

1. must contain $\vec{0}$

2. 加减乘除 right hand 都是 $0$, 所以加减乘除都在 solution set 里面

### Def: Solution Space

the solution set of a homogeneous system is called the **solution space** of the system.

## Rank

### Def: Rank

the **rank** of a matrix $A$ is the **number of leading ones** in the **RREF** of the matrix and is denoted rank $A$.

### Theorem 2.2.4

for any $m\times n$ matrix $A$ we have

$\operatorname{rank} A\le min(m,n)$

### Theorem 2.2.5 (System-Rank Theorem)

let $A$ be the coefficient matrix of a system of $m$ linear equations in $n$ unknowns $\begin{bmatrix}A&#124;\vec{b}\end{bmatrix}$.

1. the rank of $A$ is less than the rank of the augmented matrix $\begin{bmatrix}A&#124;\vec{b}\end{bmatrix}$

if and only if  the system is inconsistent.

2. if the system $\begin{bmatrix}A&#124;\vec{b}\end{bmatrix}$ is consistent, then the system contains ($n-\operatorname{rank} A$) free variables(parameters)

3. $\operatorname{rank} A=m$ if and only if the system $\begin{bmatrix}A&#124;\vec{b}\end{bmatrix}$ is consistent for every $\vec{b} \in \mathbb{R}^m$.

2 tells us that:

a consistent system $\begin{bmatrix}A&#124;\vec{b}\end{bmatrix}$

### Theorem 2.2.6 (Solution Theorem)

let $\begin{bmatrix}A&#124;\vec{b}\end{bmatrix}$ be a consistent system of $m$ linear equations in $n$ variables wuth **RREF** $\begin{bmatrix}R&#124;\vec{c}\end{bmatrix}$.

if $\operatorname{rank} A=k < n $, then a vector equation of the solution set of $\begin{bmatrix}A&#124;\vec{b}\end{bmatrix}$ has the form

$\vec{x}=\vec{d}+t_1\vec{v_1}+\cdots+t_{n-k}\vec{v_{n-k}}$,  $t_1, \dots, t_{n-k}\in \mathbb{R}$

where $\vec{d}\in \mathbb{R}^n$ and $\begin{bmatrix} \vec{v_1},\dots, \vec{n-k}\end{bmatrix}$ is a linearly independent set in $\mathbb{R}^n$.

in particular, the solution set of $\begin{bmatrix}A&#124;\vec{b}\end{bmatrix}$ is an $(n-k)$-flat in $\mathbb{R}^n$.

### Theorem 2.2.7

a set of $n$ vectors in $\mathbb{R}^n$ is linearly independent if and only if it spans $\mathbb{R}^n$ ($\operatorname{Span} n=\mathbb{R}^n$)
