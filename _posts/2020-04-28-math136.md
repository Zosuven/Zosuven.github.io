---
layout: post
title: 'MATH 136'
date: 2020-04-28
author: Zosuven
color: rgb(230,230,250)
cover: '/pics/math/cover-math136.jpg'
tags: notes math
---
{% include toc.html %}

# Chapter 1 Vectors in Euclidean Space

## large space $\mathbb{R}^n$

### Def $\mathbb{R}^n $
the set $\mathbb{R}^n $ is defined by

$\mathbb{R}^n =\begin{Bmatrix} \vec{x} = \begin{bmatrix} x_1&#92;&#92;\vdots&#92;&#92;x_n \end{bmatrix} &#124; x_1, \dots, x_n \in \mathbb{R}  \end{Bmatrix}$

two vectors $\vec{x}=\begin{bmatrix}x_1&#92;&#92;\vdots&#92;&#92;x_n\end{bmatrix}$ and $\vec{y}=\begin{bmatrix}y_1&#92;&#92;\vdots&#92;&#92;y_n\end{bmatrix}$ in $\mathbb{R}^n$ are said to be **equal**
if $x_i=y_i$ for $1\le i\le n$.


### Def Vector Addition & Scalar Multiplication

ps: vector rule for addition is sometimes called **parallelogram rule for additon**

坐标加的话就正好形成一个平行四边形

let $\vec{x}= \begin{bmatrix}x_1&#92;&#92;\vdots&#92;&#92;x_n\end{bmatrix}$ , $\vec{y}= \begin{bmatrix}y_1&#92;&#92;\vdots&#92;&#92;y_n\end{bmatrix} \in \mathbb{R}^n$ and let $c\in \mathbb{R}$ ($c$ is a **scalar**)

we define **addition** $\vec{x} +\vec{y}$, and **scalar multiplication** $c\vec{x}$, by

$\vec{x}+\vec{y}= \begin{bmatrix}
x_1+y_1&#92;&#92;\vdots&#92;&#92;x_n +y_n \end{bmatrix}$

$c\vec{x}=\begin{bmatrix}
cx_1&#92;&#92;\vdots&#92;&#92;cx_n\end{bmatrix}$


### Def linear combination

for $\vec{v_1}, \dots, \vec{v_k} \in \mathbb{R}^n $ and $c_1, \dots, c_k \in \mathbb{R}$ we call sum

$c_1\vec{v_1}+c_2\vec{v_2}+ \dots + c_k\vec{v_k}$

a **linear combination** of $\vec{v_1}, \dots, \vec{v_k}$

### Theorem1.1.1 (ten rules)

if $\vec{x}, \vec{y}, \vec{w} \in \mathbb{R}^n$ , and $c, d \in \mathbb{R}$, then

V1 $\vec{x}+\vec{y} \in \mathbb{R}^n$

V2 $(\vec{x}+\vec{y})+\vec{w}= \vec{x}+(\vec{y}+\vec{w})$

V3 $\vec{x}+\vec{y}=\vec{y}+\vec{x}$

V4 there exists a vector $\vec{0}$ such that $\vec{x}+\vec{0}=\vec{x}$ for all $\vec{x} \in \mathbb{R}^n$

V5 for every $\vec{x}\in \mathbb{R}^n$ there exists $(-\vec{x})\in \mathbb{R}^n$ such that $\vec{x}+(-\vec{x})=\vec{0}$

V6 $c\vec{x}\in \mathbb{R}^n$

V7 $c(d\vec{x})=(cd)\vec{x}$

V8 $(c+d)\vec{x}=c\vec{x}+d\vec{x}$

V9 $c(\vec{x}+\vec{y})=c\vec{x}+c\vec{y}$

V10 $1\vec{x}=\vec{x}$

conclusion:

V2, V3, V7, V8, V9 ,V10 are **operations**

V1, V4, V5, V6 are **relationship between operations and** $\mathbb{R}^n$

V4 says: exists **zero vector** $\vec{0}=\begin{bmatrix}0&#92;&#92;\vdots&#92;&#92;0\end{bmatrix}$

V5 says: exists **additive inverse** of $\vec{x}$ is $-\vec{x}=\begin{bmatrix}-x_1&#92;&#92;\vdots&#92;&#92;-x_n\end{bmatrix}$

V1 and V6 show: $\mathbb{R}^n$ is **closed under linear combination**

### Span

first:
**vector equation** is like: $\vec{x}=c\begin{Bmatrix}1&#92;&#92;1\end{Bmatrix}$

in it , $\vec{v}=\begin{Bmatrix}1&#92;&#92;1\end{Bmatrix}$ is **direction vector**

def of **span**

let $\beta = \begin{Bmatrix}\vec{v_1}, \dots,\vec{v_k}\end{Bmatrix}$ be a set of vector in $\mathbb{R}^n$.
we define the **span** of $\beta$ by

$span \beta =\begin{Bmatrix}c_1\vec{v_1}+c_2\vec{c_2}+\dots+c_k\vec{v_k} &#124; c_1, \dots, c_k \in \mathbb{R}\end{Bmatrix}$

we say that the set $\operatorname{Span} \beta$ is **spanned** by $\beta$ and that $\beta$ is a **spanning set** for $\operatorname{Span} \beta$.

#### Theorem 1.2.1

let $\vec{v_1}, \dots, \vec{v_k} \in \mathbb{R}^n$. Some vector $\vec{v_i}$, $1\le i\le k$, can be written as a linear combination of $\vec{v_1}, \dots, \vec{v_{i-1}}, \vec{v_{i+1}}, \dots, \vec{v_k}$ if and only if

$\operatorname{Span} \begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix}= \operatorname{Span} \begin{Bmatrix}\vec{v_1}, \dots,\vec{v_{i-1}}, \vec{v_{i+1}}, \dots, \vec{v_k} \end{Bmatrix}$

### Linearly Dependent & Independent

a set of vectors $\begin{Bmatrix}\vec{v_1},\dots ,\vec{v_k}\end{Bmatrix} \in \mathbb{R}^n$ is said to

**linearly dependent** if there exist coefficients $c_1, \dots, c_k$ not all zero such that

$\vec{0}=c_1\vec{v_1}+\dots+c_k\vec{c_k}$

**linearly Independent** if the only solution to

$\vec{0}=c_1\vec{v_1}+\dots+c_k\vec{v_k}$ is $c_1=c_2=\dots=c_k=0$ (called the **trivial solution**)

#### Theorem 1.2.2

a set of vectors $\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix} \in \mathbb{R}^n$ is linearly dependent if and only if
$\vec{v_i}\in \operatorname{Span} \begin{Bmatrix}\vec{v_1}, \dots, \vec{v_{i-1}}, \vec{v_{i+1}}, \dots, \vec{v_k}\end{Bmatrix}$ for some $i$, $1\le i \le k$.

#### Theorem 1.2.3

if a set of vectors $\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix}$ contains the zero vector, then it is linearly dependent


### Basis

let $S$ be a subset of $\mathbb{R}^n$

if $\begin{Bmatrix}\vec{v_1}, \dots,\vec{v_k}\end{Bmatrix}$ is a linearly independent set of vectors in $\mathbb{R}^n$ such that $S=\operatorname{Span} \begin{Bmatrix}\vec{v_1},\dots, \vec{v_k}\end{Bmatrix}$, then the set $\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix}$ is called a **basis** for $S$.

we define a basis for the set $\begin{Bmatrix}\vec{0}\end{Bmatrix}$ to be the empty set.

#### standard basis

in $\mathbb{R}^n$, let $\vec{e_i}$ represents the vector whose i-th component is $1$ and all other components are $0$. the set $\begin{Bmatrix}\vec{e_i}, \dots, \vec{e_n}\end{Bmatrix}$ is called the **standard basis** for $\mathbb{R}^n$

#### Theorem 1.2.4 (basis for a set)

if $\beta =\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix}$ is a basis for a subset $S$ of $\mathbb{R}^n$, then every vector $\vec{x} \in S$ can be written as a unique linear combination of the vector in $\beta$.


### Surfaces in Higher Dimensions

#### Def **line** in $\mathbb{R}^n$

let $\vec{v}, \vec{b} \in \mathbb{R}^n$ with $\vec{v} \neq \vec{0}$.

we call the set with vector equation $\vec{x}=c_1\vec{v}+\vec{b}$, $c_1\in \mathbb{R}$ a **line** in $\mathbb{R}^n$
through $\vec{b}$.

#### Def **Plane** in $\mathbb{R}^n$

let $\vec{v_1}, \vec{v_2}, \vec{b}\in \mathbb{R}^n$ with $\begin{Bmatrix}\vec{v_1}, \vec{v_2}\end{Bmatrix}$ being a linearly independent set.

we call the set with vector equation $\vec{x}=c_1\vec{v_1}+c_2\vec{v_2}+\vec{b}, c_1, c_2 \in \mathbb{R}$ a **plane** in $\mathbb{R}^n$ through $\vec{b}$.

#### Def **Hyperplane** in $\mathbb{R}^n$

let $\vec{v_1}, \dots, \vec{v_{n-1}}, \vec{b}\in \mathbb{R}^n$ with $\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_{n-1}}\end{Bmatrix}$ being linearly independent.

the set with vector equation $\vec{x}=c_1\vec{v_1}+\dots+c_{n-1}\vec{v_{n-1}}+\vec{b}, c_1, \dots, c_{n-1}\in \mathbb{R}$ is called a **Hyperplane** in $\mathbb{R}^n$ through $\mathbb{b}$.

#### Def k-Flat in $\mathbb{R}^n$

let $\vec{v_1},\dots, \vec{v_k}, \vec{b}\in \mathbb{R}^n$ with $\begin{Bmatrix}\vec{v_1},\dots,\vec{v_k}\end{Bmatrix}$ linearly independent.

we call the set with vector equation $\vec{x}=c_1\vec{v_1}+\dots+c_k\vec{v_k}+\vec{b}, c_1, \dots, c_k \in \mathbb{R}$ a k-flat in $\mathbb{R}^n$ through $\vec{b}$.

**a point** in $\mathbb{R}^n$ is a **0-flat**

**a line** in $\mathbb{R}^n$ is a **1-flat**

**a hyperplane** in $\mathbb{R}^n$ is an **(n-1)-flat**


## Subspaces

that is a "Space"(non-empty) which is a subset of the larger space $\mathbb{R}^n$.

### **Def** of **subspaces**:

a subset $\mathbb{S}$ of $\mathbb{R}^n$ is called a **subspace of** $\mathbb{R}$ if for every $\vec{x}, \vec{y}, \vec{w}\in \mathbb{S}$ and, $c,d\in \mathbb{R}$ we have

S1 $\vec{x}+\vec{y}\in \mathbb{S}$

S2 $(\vec{x}+\vec{y})+\vec{w}=\vec{x}+(\vec{y}+\vec{w})$

S3 $\vec{x}+\vec{y}=\vec{y}+\vec{x}$

S4 There exists a vector $\vec{0} \in \mathbb{S}$

S5 for every $\vec{x}\in \mathbb{S}$ there exists $(-\vec{x})\in \mathbb{S}$ such that $\vec{x}+(-\vec{x})=\vec{0}$

S6 $c\vec{x}\in S$

S7 $c(d\vec{x})=(cd)\vec{x}$

S8 $(c+d)\vec{x}=c\vec{x}+d\vec{x}$

S9 $c(\vec{x}+\vec{y})=c\vec{x}+c\vec{y}$

S10 $1\vec{x}=\vec{x}$

note:

S2, S3, S7, S8, S9, S10 are only about the operations of additon and scalar multiplication

if S1 and S6 hold, then for any $\vec{v} \in \mathbb{S}$, we have $\vec{0}=0\vec{v}\in \mathbb{S}$ and $(-\vec{v})=(-1)\vec{v}\in \mathbb{S}$, so S4 and S5 hold

#### Theorem 1.3.1 (Subspace Test)

let $\mathbb{S}$ be a non-empty subset of $\mathbb{R}^n$.

if $\vec{x}+\vec{y}\in \mathbb{S}$ and $c\vec{x}\in \mathbb{S}$ for all $\vec{x}, \vec{y}\in \mathbb{S}$ and $c\in \mathbb{R}$, then $\mathbb{S}$ is a subset of $\mathbb{R}^n$

#### Theorem 1.3.2

if $\vec{v_1}, \dots, \vec{v_k}\in \mathbb{R}^n$, then $\mathbb{S}=\operatorname{Span} \begin{Bmatrix}\vec{v_1},\dots, \vec{v_k}\end{Bmatrix}$ is a subspace of $\mathbb{R}^n$

implies that if a subset $\mathbb{S}$ of $\mathbb{R}^n$ has a basis, then $\mathbb{S}$ must be a subspace of $\mathbb{R}^n$

### Bases of Subspace

the procedure for finding a basis of a given subspace:

note:

1. use Theorem 1.2.1 直到 get a <span style="color:red">**linearly independent**</span> spanning set for $\mathbb{S}$

2. for bases of $\mathbb{R}^n$, every subspace of $\mathbb{R}^n$, excluding the trivial subspace $\begin{Bmatrix}\vec{0}\end{Bmatrix}$, will have infinitely many bases.

questions:

type 1:

prove that a set $\beta$ spans a subspace $\mathbb{S}$ of $\mathbb{R}^n$ ($\operatorname{Span} \beta = \mathbb{S}$)

it is to prove that $\operatorname{Span} \beta \subseteq \mathbb{S}$ and $\mathbb{S}\subseteq\operatorname{Span}\beta$

note that if all of the vectors in $\beta$ are in $\mathbb{S}$, (this step is **trivial**, and often *omitted*)

then we must have $\operatorname{Span}\beta \subseteq \mathbb{S}$,

since $\mathbb{S}$ is **closed under linear combination**

type 2:

find a basis of a given subspace

1. find the general form of a given subspace

eg. a linear equation $=c_1\vec{v_1}+\dots+c_k\vec{v_k}$ note that the $c_1,\dots, c_k\in \mathbb{R}$ are uncertain number, and the $\vec{v_1}, \dots, \vec{v_k}$ has the specific value

2. judge if the a set of vector $\vec{v_1}, \dots, \vec{v_k}$ are linearly independent: if not, making them to linearly independent

3. this set of vectors are the bases


## Dot Product

also called **standard inner product** on $\mathbb{R}^n$, or **scalar product**

recall the dot product in $\mathbb{R}^2$ and $\mathbb{R}^3$ is defined by

$\begin{bmatrix}x_1&#92;&#92;x_2\end{bmatrix}\cdot\begin{bmatrix}y_1&#92;&#92;y_2\end{bmatrix}=x_1 y_1+x_2 y_2$

$\begin{bmatrix}x_1&#92;&#92;x_2&#92;&#92;x_3\end{bmatrix}\cdot\begin{bmatrix}y_1&#92;&#92;y_2&#92;&#92;y_3\end{bmatrix}=x_1 y_1+x_2 y_2+x_3 y_3$

def: Dot Product

let $\vec{x}=\begin{bmatrix}x_1&#92;&#92;\vdots&#92;&#92;x_n\end{bmatrix}$ and $\vec{y}=\begin{bmatrix}y_1&#92;&#92;\vdots&#92;&#92;y_n\end{bmatrix}$ be vectors in $\mathbb{R}^n$.

the **dot product** of $\vec{x}$ and $\vec{y}$ is $\vec{x}\cdot\vec{y}=\begin{bmatrix}x_1&#92;&#92;\vdots&#92;&#92;x_n\end{bmatrix}$\cdot\begin{bmatrix}y_1&#92;&#92;\vdots&#92;&#92;y_n\end{bmatrix}=x_1 y_1+\cdots+x_n y_n=\sum_{i=1}^n x_i y_i$

when use?

1. calculating the **length** or **norm** of a vector $\&#124;\vec{v}\&#124;=\sqrt{\vec{v}\cdot\vec{v}}$

2. determining if two vwctors are orthogonal: if are: $\vec{x}\cdot\vec{y}=0$

3. determining the angle between two vectors (theorem 1.4.1)


### Theorem 1.4.1

if $\vec{x}, \vec{y} \in \mathbb{R}^2$ and $\theta$ is an angle between $\vec{x}$ and $\vec{y}$, then $\vec{x}\cdot\vec{y}=\&#124;\vec{x}\&#124; \&#124;\vec{y}\&#124;\cos \theta$

### Theorem 1.4.2

if $\vec{x}, \vec{y}, \vec{z}\in \mathbb{R}^n$ and $s, t\in \mathbb{R}$, then

1. $\vec{x}\cdot\vec{x}\ge 0$, and $\vec{x}\cdot\vec{x}=0$ if and only if $\vec{x}=\vec{0}$

2. $\vec{x}\vec{y}=\vec{y}\vec{x}$

3. $\vec{x}\cdot(s\vec{y}+t\vec{z})=s(\vec{x}\cdot\vec{y})+t(\vec{x}\cdot\vec{z})$

### Unit Vector

a vector $\vec{x}\in \mathbb{R}^n$ such that $\&#124;\vec{x}\&#124;=1$ is called a **Unit vector**

### Theorem 1.4.3

if $\vec{x},\vec{y}\in \mathbb{R}^n$ and $c\in \mathbb{R}$, then

1. $\&#124;\vec{x}\&#124;\ge 0$, and $\&#124;vec{x}\&#124;=\vec{0}$ if and only if $\vec{x}=\vec{0}$

2. $\&#124;c\vec{x}\&#124;=&#124;c&#124; \&#124;\vec{x}\&#124;$

3. $&#124;\vec{x}\cdot\vec{y}&#124;\le\&#124;\vec{x}\&#124; \&#124;\vec{y}\&#124;$ (Cauchy-Schwarz_Bunyakovski Inequality)

4. $\&#124;\vec{x}+\vec{y}\&#124;\le \&#124;\vec{x}\&#124; +\&#124;\vec{y}\&#124;$ (triangle Inequality)

### Def: **Angle in** $\mathbb{R}^n$

let $\vec{x}, \vec{y}\in \mathbb{R}^n$. we define an **angle** between $\vec{x}$ and $\vec{y}$ to be an angle $\theta$ such that $\vec{x}\cdot\vec{y}=\&#124;\vec{x}\&#124; \&#124;\vec{y}\&#124; \cos \theta$

### Def: **orthogonal**

let $\vec{x}, \vec{y}\in \mathbb{R}^n$. if $\vec{x}\cdot\vec{y}=0$, then $\vec{x}$ and $\vec{y}$ are said are to be **orthogonal**

### Theorem  1.4.4

the zero vector $\vec{0}\in \mathbb{R}^n$ is orthogonal to every vector $\vec{x}\in \mathbb{R}^n$.

### Def: **orthogonal set**

a set of vectors $\begin{Bmatrix}\vec{v_1},\cdots,\vec{v_k}\end{Bmatrix}$ in $\mathbb{R}^n$ is called an **orthogonal set** if $\vec{v_i}\cdot\vec{v_j}=0$ for all $i\neq j$.

note: a set of unit vectors is an orthogonal set.

## Cross Product

cross product only works when $\mathbb{R}^3$

let $\vec{v}, \vec{w}\in \mathbb{R}^3$.

the **cross product** of $\vec{v}=\begin{bmatrix}v_1&#92;&#92;v_2&#92;&#92;v_3\end{bmatrix}$ and $\vec{w}=\begin{bmatrix}w_1&#92;&#92;w_2&#92;&#92;w_3\end{bmatrix}$ is said to be

$\vec{v}\times\vec{w}=\begin{bmatrix}v_2w_3-v_3w_2&#92;&#92;v_3w_1-v_1w_3&#92;&#92;v_1w_2-v_2w_1\end{bmatrix}$

### Theorem 1.4.5

Suppose that $\vec{v}, \vec{w}, \vec{x}\in \mathbb{R}^3 and c\in \mathbb{R}$.

1. if $\vec{n}=\vec{v}\times\vec{w}$, then for any $\vec{y}\in \operatorname{Span}\begin{Bmatrix}\vec{v},\vec{w}\end{Bmatrix}$ we have $\vec{y}\cdot\vec{n}=0$

$\vec{n}$ actually is the orthogonal vector of the plane produced by the $\vec{v}\times\vec{w}$

2.$\vec{v}\time\vec{w}=-\vec{w}\times\vec{v}$

3. $\vec{v}\times\vec{v}=\vec{0}$

4. $\vec{v}\times\vec{w}=\vec{0}$ if and only if either $\vec{v}=\vec{0}$ or $\vec{w}$ is a scalar muitiple of $\vec{v}$

5. $\vec{v}\times(\vec{w}+\vec{x})=\vec{v}\times\vec{w}+\vec{v}\times\vec{x}$

6. $(c\vec{v})\times(\vec{w})=c(\vec{v}\times\vec{w})$

7. $\&#124;\vec{v}\times\vec{w}\&#124;=\&#124;\vec{v}\&#124; \&#124;\vec{w}\&#124 &#124\sin \theta &#124;$ where $\theta$ is the angle between $\vec{v}$ and $\vec{w}$

## Scalar Equation of a Plane

### Theorem 1.4.6

let $\vec{v}, \vec{w}, \vec{b}\in \mathbb{R}^3$ with $\begin{Bmatrix}\vec{v}, \vec{w}\end{Bmatrix}$ being linearly independent and

let $P$ be a plane in $\mathbb{R}^3$ with vector equation $\vec{x}=s\vec{v}+t\vec{w}+\vec{b}, s, t\in \mathbb{R}$

if $\vec{n}=\vec{v}\times\vec{w}$, then an equation for the plane is $(\vec{x}-\vec{b})\cdot\vec{n}=0$

### Def: Normal Vector & Scalar equation

let $P$ be a plane in $\mathbb{R}^3$ that passes through the point $B(b_1, b_2, b_3)$. If $\vec{n}=\begin{bmatrix}n_1&#92;&#92;n_2&#92;&#92;n_3\end{bmatrix}\in \mathbb{R}^3$ is a vector such that

(the form: $\vec{x}\cdot\vec{n}=\vec{b}\cdot\vec{n}$)

$n_1x_1+n_2x_2+n_3x_3=b_1n_1+b_2n_2+b_3n_3$ (1)

is an equation for $P$, then $\vec{n}$ is called a **normal vector** for $P$. we call equation (1) a **scalar equation** of $P$

## Scalar Equation of Hyperplane

If $\begin{Bmatrix}\vec{v_1}, \dots,\vec{v_{m-1}}\end{Bmatrix}$ is a linearly independent set of vectors in $\mathbb{R}^m , m\ge 2$ , and $\vec{b}\in \mathbb{R}^m$,

then , $\vec{x}=c_1\vec{v_1}+\cdots+c_{m-1}\vec{v_{m-1}}+\vec{b}$ is a vector equation of a hyperplane in $\mathbb{R}^m$.

if there exists a non-zero vector $\vec{n}=\begin{bmatrix}n_1&#92;&#92;\vdots&#92;&#92;n_m\end{bmatrix}\in \mathbb{R}^m$ which is orthogonal to each of $\vec{v_1}, \dosts, \vec{v_{m-1}}$,

then (with the form $\vec{x}\cdot\vec{n}=\vec{b}\cdot\vec{n}$) the **scalar equation** is

$n_1x_1+\cdots+n_mx_m=n_1b_1+\cdots+n_mb_m$

## Projections

### Def: **projection** onto a **Line** in $\mathbb{R}^n$

let $\vec{u}, \vec{v}\in \mathbb{R}^n$ with $\vec{v}\neq \vec{0}$. the **projection** of $\vec{u}$ onto $\vec{v}$ is defined by

$\operatorname{proj}_{\vec{v}} (\vec{u})=\frac{\vec{u}\cdot\vec{v}}{\&#124;\vec{v}\&#124;}^2 \vec{v}$

### Def: **Perpendicular** of a Projection in $\mathbb{R}^n$

let $\vec{u}, \vec{v} \in \mathbb{R}^n$ with $\vec{v}\neq \vec{0}$. the **Perpendicular** of $\vec{u}$ into $\vec{v}$ is defined by

$\operatorname{perp}_{\vec{v}} (\vec{u})=\vec{u}-\operatorname{proj}_{\vec{v}}(\vec{u})$

### Def: **Projection** and **Perpendicular** onto a **Plane** in $\mathbb{R}^3$

let $P$ be a plane in $\mathbb(R)^3$ that passes through the origin and has normal vector $\vec{n}$,

the **projection** of $\vec{x}\in \mathbb(R)^3$ onto $P$ is defined by  $\operatorname{proj}_{P} (\vec{x})=\operatorname{perp}_{\vec{n}} (\vec{x})$

the **perpendicular** of $\vec{x}$ onto $P$ is defined by $\operatorname{perp}_{P} (\vec{x})=\operatorname{proj}_{\vec{n}} (\vec{x})$
