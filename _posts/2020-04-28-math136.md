---
layout: post
title: 'MATH 136'
date: 2020-04-28
author: Zosuven
color: rgb(230,230,250)
cover: '/pics/math/cover-math136.jpg'
tags: notes math
---
{% include toc.html %}

# Chapter 1 Vectors in Euclidean Space

## large space $\mathbb{R}^n$

### Def $\mathbb{R}^n $
the set $\mathbb{R}^n $ is defined by

$\mathbb{R}^n =\begin{Bmatrix} \vec{x} = \begin{bmatrix} x_1&#92;&#92;\vdots&#92;&#92;x_n \end{bmatrix} &#124; x_1, \dots, x_n \in \mathbb{R}  \end{Bmatrix}$

two vectors $\vec{x}=\begin{bmatrix}x_1&#92;&#92;\vdots&#92;&#92;x_n\end{bmatrix}$ and $\vec{y}=\begin{bmatrix}y_1&#92;&#92;\vdots&#92;&#92;y_n\end{bmatrix}$ in $\mathbb{R}^n$ are said to be **equal**
if $x_i=y_i$ for $1\le i\le n$.


### Def Vector Addition & Scalar Multiplication

ps: vector rule for addition is sometimes called **parallelogram rule for additon**

坐标加的话就正好形成一个平行四边形

let $\vec{x}= \begin{bmatrix}x_1&#92;&#92;\vdots&#92;&#92;x_n\end{bmatrix}$ , $\vec{y}= \begin{bmatrix}y_1&#92;&#92;\vdots&#92;&#92;y_n\end{bmatrix} \in \mathbb{R}^n$ and let $c\in \mathbb{R}$ ($c$ is a **scalar**)

we define **addition** $\vec{x} +\vec{y}$, and **scalar multiplication** $c\vec{x}$, by

$\vec{x}+\vec{y}= \begin{bmatrix}
x_1+y_1&#92;&#92;\vdots&#92;&#92;x_n +y_n \end{bmatrix}$

$c\vec{x}=\begin{bmatrix}
cx_1&#92;&#92;\vdots&#92;&#92;cx_n\end{bmatrix}$


### Def linear combination

for $\vec{v_1}, \dots, \vec{v_k} \in \mathbb{R}^n $ and $c_1, \dots, c_k \in \mathbb{R}$ we call sum

$c_1\vec{v_1}+c_2\vec{v_2}+ \dots + c_k\vec{v_k}$

a **linear combination** of $\vec{v_1}, \dots, \vec{v_k}$

### Theorem (ten rules)

if $\vec{x}, \vec{y}, \vec{w} \in \mathbb{R}^n$ , and $c, d \in \mathbb{R}$, then

V1 $\vec{x}+\vec{y} \in \mathbb{R}^n$

V2 $(\vec{x}+\vec{y})+\vec{w}= \vec{x}+(\vec{y}+\vec{w})$

V3 $\vec{x}+\vec{y}=\vec{y}+\vec{x}$

V4 there exists a vector $\vec{0}$ such that $\vec{x}+\vec{0}=\vec{x}$ for all $\vec{x} \in \mathbb{R}^n$

V5 for every $\vec{x}\in \mathbb{R}^n$ there exists $(-\vec{x})\in \mathbb{R}^n$ such that $\vec{x}+(-\vec{x})=\vec{0}$

V6 $c\vec{x}\in \mathbb{R}^n$

V7 $c(d\vec{x})=(cd)\vec{x}$

V8 $(c+d)\vec{x}=c\vec{x}+d\vec{x}$

V9 $c(\vec{x}+\vec{y})=c\vec{x}+c\vec{y}$

V10 $1\vec{x}=\vec{x}$

conclusion:

V2, V3, V7, V8, V9 ,V10 are **operations**

V1, V4, V5, V6 are **relationship between operations and** $\mathbb{R}^n$

V4 says: exists **zero vector** $\vec{0}=\begin{bmatrix}0&#92;&#92;\vdots&#92;&#92;0\end{bmatrix}$

V5 says: exists **additive inverse** of $\vec{x}$ is $-\vec{x}=\begin{bmatrix}-x_1&#92;&#92;\vdots&#92;&#92;-x_n\end{bmatrix}$

V1 and V6 show: $\mathbb{R}^n$ is **closed under linear combination**

### Span

first:
**vector equation** is like: $\vec{x}=c\begin{Bmatrix}1&#92;&#92;1\end{Bmatrix}$

in it , $\vec{v}=\begin{Bmatrix}1&#92;&#92;1\end{Bmatrix}$ is **direction vector**

def of **span**

let $\beta = \begin{Bmatrix}\vec{v_1}, \dots,\vec{v_k}\end{Bmatrix}$ be a set of vector in $\mathbb{R}^n$.
we define the **span** of $\beta$ by

$span \beta =\begin{Bmatrix}c_1\vec{v_1}+c_2\vec{c_2}+\dots+c_k\vec{v_k} &#124; c_1, \dots, c_k \in \mathbb{R}\end{Bmatrix}$

we say that the set $\operatorname{Span} \beta$ is **spanned** by $\beta$ and that $\beta$ is a **spanning set** for $\operatorname{Span} \beta$.

#### Theorem 1.1.1

let $\vec{v_1}, \dots, \vec{v_k} \in \mathbb{R}^n$. Some vector $\vec{v_i}$, $1\le i\le k$, can be written as a linear combination of $\vec{v_1}, \dots, \vec{v_{i-1}}, \vec{v_{i+1}}, \dots, \vec{v_k}$ if and only if

$\operatorname{Span} \begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix}= \operatorname{Span} \begin{Bmatrix}\vec{v_1}, \dots,\vec{v_{i-1}}, \vec{v_{i+1}}, \dots, \vec{v_k} \end{Bmatrix}$

### Linearly Dependent & Independent

a set of vectors $\begin{Bmatrix}\vec{v_1},\dots ,\vec{v_k}\end{Bmatrix} \in \mathbb{R}^n$ is said to

**linearly dependent** if there exist coefficients $c_1, \dots, c_k$ not all zero such that

$\vec{0}=c_1\vec{v_1}+\dots+c_k\vec{c_k}$

**linearly Independent** if the only solution to

$\vec{0}=c_1\vec{v_1}+\dots+c_k\vec{v_k}$ is $c_1=c_2=\dots=c_k=0$ (called the **trivial solution**)

#### Theorem 1.1.2

a set of vectors $\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix} \in \mathbb{R}^n$ is linearly dependent if and only if
$\vec{v_i}\in \operatorname{Span} \begin{Bmatrix}\vec{v_1}, \dots, \vec{v_{i-1}}, \vec{v_{i+1}}, \dots, \vec{v_k}\end{Bmatrix}$ for some $i$, $1\le i \le k$.

#### Theorem 1.1.3

if a set of vectors $\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix}$ contains the zero vector, then it is linearly dependent


### Basis

let $S$ be a subset of $\mathbb{R}^n$

if $\begin{Bmatrix}\vec{v_1}, \dots,\vec{v_k}\end{Bmatrix}$ is a linearly independent set of vectors in $\mathbb{R}^n$ such that $S=\operatorname{Span} \begin{Bmatrix}\vec{v_1},\dots, \vec{v_k}\end{Bmatrix}$, then the set $\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix}$ is called a **basis** for $S$.

we define a basis for the set $\begin{Bmatrix}\vec{0}\end{Bmatrix}$ to be the empty set.

#### standard basis

in $\mathbb{R}^n$, let $\vec{e_i}$ represents the vector whose i-th component is $1$ and all other components are $0$. the set $\begin{Bmatrix}\vec{e_i}, \dots, \vec{e_n}\end{Bmatrix}$ is called the **standard basis** for $\mathbb{R}^n$

#### Theorem 1.1.4 (basis for a set)

if $\beta =\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_k}\end{Bmatrix}$ is a basis for a subset $S$ of $\mathbb{R}^n$, then every vector $\vec{x} \in S$ can be written as a unique linear combination of the vector in $\beta$.


### Surfaces in Higher Dimensions

#### Def **line** in $\mathbb{R}^n$

let $\vec{v}, \vec{b} \in \mathbb{R}^n$ with $\vec{v} \neq \vec{0}$.

we call the set with vector equation $\vec{x}=c_1\vec{v}+\vec{b}$, $c_1\in \mathbb{R}$ a **line** in $\mathbb{R}^n$
through $\vec{b}$.

#### Def **Plane** in $\mathbb{R}^n$

let $\vec{v_1}, \vec{v_2}, \vec{b}\in \mathbb{R}^n$ with $\begin{Bmatrix}\vec{v_1}, \vec{v_2}\end{Bmatrix}$ being a linearly independent set.

we call the set with vector equation $\vec{x}=c_1\vec{v_1}+c_2\vec{v_2}+\vec{b}, c_1, c_2 \in \mathbb{R}$ a **plane** in $\mathbb{R}^n$ through $\vec{b}$.

#### Def **Hyperplane** in $\mathbb{R}^n$

let $\vec{v_1}, \dots, \vec{v_{n-1}}, \vec{b}\in \mathbb{R}^n$ with $\begin{Bmatrix}\vec{v_1}, \dots, \vec{v_{n-1}}\end{Bmatrix}$ being linearly independent.

the set with vector equation $\vec{x}=c_1\vec{v_1}+\dots+c_{n-1}\vec{v_{n-1}}+\vec{b}, c_1, \dots, c_{n-1}\in \mathbb{R}$ is called a **Hyperplane** in $\mathbb{R}^n$ through $\mathbb{b}$.

#### Def k-Flat in $\mathbb{R}^n$

let $\vec{v_1},\dots, \vec{v_k}, \vec{b}\in \mathbb{R}^n$ with $\begin{Bmatrix}\vec{v_1},\dots,\vec{v_k}\end{Bmatrix}$ linearly independent.

we call the set with vector equation $\vec{x}=c_1\vec{v_1}+\dots+c_k\vec{v_k}+\vec{b}, c_1, \dots, c_k \in \mathbb{R}$ a k-flat in $\mathbb{R}^n$ through $\vec{b}$.

**a point** in $\mathbb{R}^n$ is a **0-flat**

**a line** in $\mathbb{R}^n$ is a **1-flat**

**a hyperplane** in $\mathbb{R}^n$ is an **(n-1)-flat**


## Subspaces

that is a "Space"(non-empty) which is a subset of the larger space $\mathbb{R}^n$.

### **Def** of **subspaces**:

a subset $\mathbb{S}$ of $\mathbb{R}^n$ is called a **subspace of** $\mathbb{R}$ if for every $\vec{x}, \vec{y}, \vec{w}\in \mathbb{S}$ and, $c,d\in \mathbb{R}$ we have

S1 $\vec{x}+\vec{y}\in \mathbb{S}$

S2 $(\vec{x}+\vec{y})+\vec{w}=\vec{x}+(\vec{y}+\vec{w})$

S3 $\vec{x}+\vec{y}=\vec{y}+\vec{x}$

S4 There exists a vector $\vec{0} \in \mathbb{S}$

S5 for every $\vec{x}\in \mathbb{S}$ there exists $(-\vec{x})\in \mathbb{S}$ such that $\vec{x}+(-\vec{x})=\vec{0}$

S6 $c\vec{x}\in S$

S7 $c(d\vec{x})=(cd)\vec{x}$

S8 $(c+d)\vec{x}=c\vec{x}+d\vec{x}$

S9 $c(\vec{x}+\vec{y})=c\vec{x}+c\vec{y}$

S10 $1\vec{x}=\vec{x}$

note:

S2, S3, S7, S8, S9, S10 are only about the operations of additon and scalar multiplication

if S1 and S6 hold, then for any $\vec{v} \in \mathbb{S}$, we have $\vec{0}=0\vec{v}\in \mathbb{S}$ and $(-\vec{v})=(-1)\vec{v}\in \mathbb{S}$, so S4 and S5 hold

#### Theorem 1.2.1 (Subspace Test)

let $\mathbb{S}$ be a non-empty subset of $\mathbb{R}^n$.

if $\vec{x}+\vec{y}\in \mathbb{S}$ and $c\vec{x}\in \mathbb{S}$ for all $\vec{x}, \vec{y}\in \mathbb{S}$ and $c\in \mathbb{R}$, then $\mathbb{S}$ is a subset of $\mathbb{R}^n$

#### Theorem 1.2.2

if $\vec{v_1}, \dots, \vec{v_k}\in \mathbb{R}^n$, then $\mathbb{S}=\operatorname{Span} \begin{Bmatrix}\vec{v_1},\dots, \vec{v_k}\end{Bmatrix}$ is a subspace of $\mathbb{R}^n$

implies that if a subset $\mathbb{S}$ of $\mathbb{R}^n$ has a basis, then $\mathbb{S}$ must be a subspace of $\mathbb{R}^n$

### Bases of Subspace

the procedure for finding a basis of a given subspace:

note:

1. use Theorem 1.2.1 直到 get a <span style="color:red">**linearly independent**</span> spanning set for $\mathbb{S}$

2. for bases of $\mathbb{R}^n$, every subspace of $\mathbb{R}^n$, excluding the trivial subspace $\begin{Bmatrix}\vec{0}\end{Bmatrix}$, will have infinitely many bases.

questions:

type 1:

prove that a set $\beta$ spans a subspace $\mathbb{S}$ of $\mathbb{R}^n$ ($\operatorname{Span} \beta = \mathbb{S}$)

it is to prove that $\operatorname{Span} \beta \subseteq \mathbb{S}$ and $\mathbb{S}\subseteq\operatorname{Span}\beta$

note that if all of the vectors in $\beta$ are in $\mathbb{S}$, (this step is **trivial**, and often *omitted*)

then we must have $\operatorname{Span}\beta \subseteq \mathbb{S}$,

since $\mathbb{S}$ is **closed under linear combination**

type 2:

find a basis of a given subspace

1. find the general form of a given subspace

eg. a linear equation $=c_1\vec{v_1}+\dots+c_k\vec{v_k}$ note that the $c_1,\dots, c_k\in \mathbb{R}$ are uncertain number, and the $\vec{v_1}, \dots, \vec{v_k}$ has the specific value

2. judge if the a set of vector $\vec{v_1}, \dots, \vec{v_k}$ are linearly independent: if not, making them to linearly independent

3. this set of vectors are the bases
